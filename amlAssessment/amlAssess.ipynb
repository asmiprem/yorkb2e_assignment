{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@localhost:5432/postgres')\n",
    "\n",
    "student_data= pd.read_csv('data.csv',delimiter=';')\n",
    "student_data.to_sql('students', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_student(new_student):\n",
    "    new_student_df = pd.DataFrame(new_student)\n",
    "    new_student_df.to_sql('students', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_student_dict = {\n",
    "    'school': 'GP',\n",
    "    'sex': 'F',\n",
    "    'age': 17,\n",
    "    'address': 'U',\n",
    "    'famsize': 'GT3',\n",
    "    'Pstatus': 'T',\n",
    "    'Medu': 4,\n",
    "    'Fedu': 3,\n",
    "    'Mjob': 'health',\n",
    "    'Fjob': 'teacher',\n",
    "    'reason': 'home',\n",
    "    'guardian': 'mother',\n",
    "    'traveltime': 2,\n",
    "    'studytime': 3,\n",
    "    'failures': 0,\n",
    "    'schoolsup': 'no',\n",
    "    'famsup': 'yes',\n",
    "    'paid': 'no',\n",
    "    'activities': 'yes',\n",
    "    'nursery': 'yes',\n",
    "    'higher': 'yes',\n",
    "    'internet': 'yes',\n",
    "    'romantic': 'no',\n",
    "    'famrel': 5,\n",
    "    'freetime': 4,\n",
    "    'goout': 3,\n",
    "    'Dalc': 2,\n",
    "    'Walc': 3,\n",
    "    'health': 4,\n",
    "    'absences': 1,\n",
    "    'G1': 15,\n",
    "    'G2': 14,\n",
    "    'G3': 16\n",
    "}\n",
    "add_student([new_student_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
      "0       GP   F   16       U     LE3       T     4     3   teacher  services   \n",
      "1       GP   M   18       U     LE3       T     1     1     other     other   \n",
      "2       GP   M   17       R     LE3       A     4     4   teacher     other   \n",
      "3       GP   F   15       U     LE3       T     3     2  services     other   \n",
      "4       GP   M   16       U     GT3       T     2     3     other     other   \n",
      "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
      "311     GP   M   15       U     LE3       A     2     1  services     other   \n",
      "312     GP   F   17       U     GT3       A     4     4     other   teacher   \n",
      "313     GP   F   15       U     GT3       T     4     4   teacher   teacher   \n",
      "314     MS   F   19       R     GT3       T     2     3  services     other   \n",
      "315     GP   F   16       U     GT3       T     4     3     other   at_home   \n",
      "\n",
      "     ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
      "0    ...      5        4      3     1     2      1        2  16  15  15  \n",
      "1    ...      2        3      5     2     5      4        0   6   5   0  \n",
      "2    ...      3        3      3     2     3      4        2  10  11  12  \n",
      "3    ...      4        4      4     1     1      5       10   7   6   6  \n",
      "4    ...      5        3      3     1     1      3        0  13  14  14  \n",
      "..   ...    ...      ...    ...   ...   ...    ...      ...  ..  ..  ..  \n",
      "311  ...      4        5      5     2     5      5        0   8   9  10  \n",
      "312  ...      4        1      4     1     1      1        6   6   5   6  \n",
      "313  ...      4        3      2     1     1      5        0  16  16  15  \n",
      "314  ...      5        4      2     1     2      5        0   7   5   0  \n",
      "315  ...      5        3      5     1     1      3        0   7   9   8  \n",
      "\n",
      "[316 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "student_df = pd.read_csv(\"data.csv\",delimiter=';')\n",
    "print(student_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features=student_df.select_dtypes(include=['object']).columns\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_featur=student_df.select_dtypes(exclude=['object'])\n",
    "num_featur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a correlation matrix for selected columns\n",
    "correlation_matrix_selected = num_featur.corr()\n",
    "\n",
    "# Plot heatmap for selected columns\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(correlation_matrix_selected, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Selected Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(student_df, drop_first=True)\n",
    "print(data_encoded.info)\n",
    "print(data_encoded.shape)\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(16, 12))\n",
    "correlation_matrix = data_encoded.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for correlation matrix\n",
    "selected_columns = ['G1', 'G2', 'G3', 'studytime', 'failures', 'absences']\n",
    "selected_data = student_df[selected_columns]\n",
    "# Creating a correlation matrix for selected columns\n",
    "correlation_matrix_selected = selected_data .corr()\n",
    "\n",
    "# Plot heatmap for selected columns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix_selected, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Selected Columns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded .hist(bins = 25, figsize = (15,10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =data_encoded.copy()\n",
    "y = X.pop(\"G3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "print(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores,color='salmon')\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(16, 10))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(X_test, y_test, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mean_absolute_error_s = mean_absolute_error(y_test, y_pred)\n",
    "    evaluation_results = {\n",
    "        'Model': model,\n",
    "        'R2 Score': r2,\n",
    "        'Mean Squared Error': mse,\n",
    "        'Mean Absolute Error': mean_absolute_error_s\n",
    "    }\n",
    "   \n",
    "    df_result = pd.DataFrame([evaluation_results])\n",
    "    \n",
    "    return df_result    \n",
    "def visualize_evaluation(df_result):\n",
    "    # Extract model names and evaluation metrics\n",
    "    models = df_result['Model']\n",
    "    r2_scores = df_result['R2 Score']\n",
    "    mse_values = df_result['Mean Squared Error']\n",
    "    mae_values = df_result['Mean Absolute Error']\n",
    "\n",
    "    # Create a bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    width = 0.2\n",
    "\n",
    "    ax.bar(models, r2_scores, width, label='R2 Score')\n",
    "    ax.bar(models, mse_values, width, label='Mean Squared Error', bottom=r2_scores)\n",
    "    ax.bar(models, mae_values, width, label='Mean Absolute Error', bottom=r2_scores + mse_values)\n",
    "\n",
    "    ax.set_ylabel('Metrics')\n",
    "    ax.set_title('Model Evaluation Metrics')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    param_results = {\n",
    "        'best_params': best_params,\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'result': results_df\n",
    "    }\n",
    "    return param_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, models):\n",
    "    X = data.drop(\"G3\", axis=1)\n",
    "    y = data[\"G3\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_preprocessed = pd.get_dummies(X_train, drop_first=True)\n",
    "    X_test_preprocessed = pd.get_dummies(X_test, drop_first=True)\n",
    "    \n",
    "    tuned_results = []\n",
    "    untuned_results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(param_grids[model_name])\n",
    "        tuning_results = tune_hyperparameters(model, param_grids[model_name], X_train_preprocessed, y_train)\n",
    "        best_params, best_model, best_score,  results_df = tuning_results['best_params'], tuning_results['best_model'], tuning_results['best_score'],tuning_results[results_df]\n",
    "\n",
    "        print(f\"{model_name} - Best Params: {best_params}, Best Score: {best_score}\")\n",
    "        tuned_results = evaluate_model(X_test_preprocessed, y_test, best_model)\n",
    "        \n",
    "     \n",
    "        print(f\"{model_name} - Tuned Results: {tuned_results}\")\n",
    "\n",
    "        # Fit the best model\n",
    "        model.fit(X_train_preprocessed, y_train)\n",
    "        untuned_results = evaluate_model(X_test_preprocessed, y_test, model)\n",
    "        print(f\"{model_name} - Untuned Results: {untuned_results}\")\n",
    "\n",
    "    return tuned_results,untuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 200, 500], 'criterion': ['squared_error', 'absolute_error'], 'min_samples_split': [2, 3, 4, 5], 'min_samples_leaf': [1, 2, 4, 5], 'max_leaf_nodes': [4, 10, 20, 50, None]}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'results_df' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 35\u001b[0m\n\u001b[1;32m     28\u001b[0m param_grids \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForestRegressor\u001b[39m\u001b[38;5;124m'\u001b[39m: randomForest_param_grid, \n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear SVR\u001b[39m\u001b[38;5;124m'\u001b[39m: svr_param_grid,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge Regressor\u001b[39m\u001b[38;5;124m'\u001b[39m: ridge_param_grid\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m df1 \u001b[38;5;241m=\u001b[39m student_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 35\u001b[0m tuned_df1,untuned_df1\u001b[38;5;241m=\u001b[39m\u001b[43mdata_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_selected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(tuned_df1,untuned_df1)\n\u001b[1;32m     37\u001b[0m selection \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mdata_process\u001b[0;34m(data, models)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(param_grids[model_name])\n\u001b[1;32m     13\u001b[0m tuning_results \u001b[38;5;241m=\u001b[39m tune_hyperparameters(model, param_grids[model_name], X_train_preprocessed, y_train)\n\u001b[0;32m---> 14\u001b[0m best_params, best_model, best_score,  results_df \u001b[38;5;241m=\u001b[39m tuning_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m], tuning_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m'\u001b[39m], tuning_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m'\u001b[39m],tuning_results[\u001b[43mresults_df\u001b[49m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Best Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m tuned_results \u001b[38;5;241m=\u001b[39m evaluate_model(X_test_preprocessed, y_test, best_model)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'results_df' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "svr_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'epsilon': [0.01, 0.1, 0.2],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "#svr_model = SVR()\n",
    "\n",
    "\n",
    "ridge_param_grid = {'alpha': [0.1, 1, 10]} #default=1.0\n",
    "#ridge_model = Ridge()\n",
    "\n",
    "randomForest_param_grid={\n",
    "    'n_estimators':[100,200,500],  #default 100\n",
    "    'criterion':['squared_error','absolute_error'], #default squared_error\n",
    "    'min_samples_split':[2,3,4,5], #default 2\n",
    "    'min_samples_leaf':[1,2,4,5], #default=1\n",
    "    'max_leaf_nodes':[4,10,20,50,None] #default=None\n",
    "}\n",
    "#randomForest_model=RandomForestRegressor()\n",
    "# Dictionary of models\n",
    "models_selected = {\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'Linear SVR': SVR(),\n",
    "    'Ridge Regressor': Ridge()\n",
    "}\n",
    "\n",
    "# Dictionary of param grids\n",
    "param_grids = {\n",
    "    'RandomForestRegressor': randomForest_param_grid, \n",
    "    'Linear SVR': svr_param_grid,\n",
    "    'Ridge Regressor': ridge_param_grid\n",
    "}\n",
    "\n",
    "df1 = student_df.copy()\n",
    "tuned_df1,untuned_df1=data_process(df1, models_selected)\n",
    "print(tuned_df1,untuned_df1)\n",
    "selection = [\"G1\", \"G2\", \"G3\"]\n",
    "df2 = df1[selection].copy()\n",
    "tuned_df2,untuned_df2=data_process(df2, models_selected)\n",
    "print(tuned_df2,untuned_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m tuned_r2_scores \u001b[38;5;241m=\u001b[39m results_df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTuned R2 Score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m untuned_r2_scores \u001b[38;5;241m=\u001b[39m results_df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUntuned R2 Score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "models = results_df1['Model']\n",
    "tuned_r2_scores = results_df1['Tuned R2 Score']\n",
    "untuned_r2_scores = results_df1['Untuned R2 Score']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "\n",
    "bar1 = plt.bar(models, tuned_r2_scores, width=bar_width, label='Tuned R2 Score')\n",
    "bar2 = plt.bar(models, untuned_r2_scores, width=bar_width, label='Untuned R2 Score', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('Tuned and Untuned R2 Scores for Different Models')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
