{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extraction completed Successfully\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/\"\n",
    "start_year =1950\n",
    "end_year = 2000\n",
    "for year in range(start_year, end_year + 1):\n",
    "    # Create a directory for the current year\n",
    "    year_directory = f\"./temp/{year}/\"\n",
    "    os.makedirs(year_directory, exist_ok=True)\n",
    "\n",
    "    # Construct the URL for the current year\n",
    "    url = f\"{base_url}{year}.tar.gz\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with tarfile.open(fileobj=response.raw, mode=\"r|gz\") as file:\n",
    "        # Extract all contents to the year-specific directory\n",
    "            file.extractall(year_directory)\n",
    "            print(\"Data Extraction completed Successfully\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {year}. Status code: {response.status_code}\")\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "year_folders = glob.glob(os.path.join(path, \"./temp/*/\"))\n",
    "\n",
    "dfs = []\n",
    "#to loop year folder\n",
    "for year_folder in year_folders:\n",
    "    csv_pattern = os.path.join(year_folder, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "   \n",
    "\n",
    "    prefixes_to_include = ('7', '99', '69')\n",
    "    prefix_to_exclude = ('76', '71')\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        file_name = os.path.basename(csv_file)\n",
    "        if file_name.startswith(prefixes_to_include) and not file_name.startswith(prefix_to_exclude):\n",
    "    \n",
    "   \n",
    "            \n",
    "            df = pd.read_csv(csv_file)\n",
    "\n",
    "           #to Check for US location but exclude 'Hawaii' and 'Alaska'\n",
    "            df = df[\n",
    "                (df['LATITUDE'] >= 24.396308) & (df['LATITUDE'] <= 49.384358) &\n",
    "                (df['LONGITUDE'] >= -125.000000) & (df['LONGITUDE'] <= -66.934570)\n",
    "            ]\n",
    "            df=df[['DATE','PRCP','TEMP','MIN','MAX','LATITUDE','LONGITUDE','NAME']]\n",
    "            # Appending the DataFrame to the list\n",
    "            dfs.append(df)\n",
    "\n",
    "# Concatenating the list of DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "#Delete the temp folder\n",
    "shutil.rmtree('./temp')\n",
    "\n",
    "merged_df.to_csv('merged_data_filtered.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['DATE'] = pd.to_datetime(merged_df['DATE'])\n",
    "merged_df['MONTH'] = merged_df['DATE'].dt.month\n",
    "merged_df['YEAR'] = merged_df['DATE'].dt.year\n",
    "df_temp = merged_df[merged_df['TEMP'] != 9999.9]\n",
    "result_temp = df_temp.groupby(['YEAR', 'MONTH'])['TEMP'].agg(['mean', 'median', 'var', 'min', 'max']).reset_index()\n",
    "df_prcp = merged_df[merged_df['PRCP'] != 99.99]\n",
    "result_prcp = df_prcp.groupby(['YEAR', 'MONTH'])['PRCP'].agg(['mean', 'median', 'var', 'min', 'max']).reset_index()\n",
    "weather_stats= pd.merge(result_temp, result_prcp, on=['YEAR', 'MONTH'], suffixes=('_temp', '_prcp'))\n",
    "weather_stats.to_csv('monthly_weather_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load to Postgres SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('postgresql://postgres:password@localhost:5432/postgres')\n",
    "\n",
    "monthly_weather_stats = pd.read_csv('monthly_weather_stats.csv')\n",
    "monthly_weather_stats.to_sql('monthly_weather_stats', con=engine, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
