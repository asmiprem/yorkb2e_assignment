{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'Listing', 'data': {'after': 't3_18o0rrw', 'dist': 26, 'modhash': '', 'geo_filter': None, 'children': [{'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\\n\\nThread will stay alive until next one so keep posting after the date in the title.\\n\\nThanks to everyone for answering questions in the previous thread!', 'author_fullname': 't2_6l4z3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Simple Questions Thread', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18kkdbb', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1702828819.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!&lt;/p&gt;\\n\\n&lt;p&gt;Thread will stay alive until next one so keep posting after the date in the title.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks to everyone for answering questions in the previous thread!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'new', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18kkdbb', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'AutoModerator', 'discussion_type': None, 'num_comments': 26, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/', 'parent_whitelist_status': 'all_ads', 'stickied': True, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/', 'subreddit_subscribers': 2843382, 'created_utc': 1702828819.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': \"[Nero10578/Mistral-7B-Sunda-v1.0 Â· Hugging Face](https://huggingface.co/Nero10578/Mistral-7B-Sunda-v1.0)\\n\\nI'll start by saying I am not a machine learning expert and I am new to this since getting into LLMs as it got popular since LLaMa release. So, I don't know much of the technicalities although I am willing to learn.\\n\\nSeeing that even Bing chat which is powered by chatGPT-4 couldn't speak in Sundanese when asked, I thought of trying to teach Mistral-7B Sundanese using just QLora training. It surprisingly worked out pretty well for how little data I had to train it with.\\n\\nWhy Sundanese? Because I can speak it and it is a regional language in Indonesia that isn't used much if at all on the internet so there was basically almost no chance it was trained well on any of these LLM models coming out.\\n\\nThis is more of an exercise to see if a small open-source model like Mistral 7B can be trained to learn a new very obscure language that is apparently never there in the original dataset. More details on the dataset and training settings are in the huggingface model card. Please ask me if you have any questions. Also, I have no idea how I am supposed to name the version, but I just called this 1.0 because the 0.x versions are basically me trial and erroring burning up electricity and heating my place for quite a while.\\n\\nThe resulting model is actually surprisingly coherent and translates knowledge in English to Sundanese decently. Although it is a bit unstable in that sometimes it replies with incorrect information or irrelevant information, which is probably because of the limited dataset. Next steps are going to be to get more datasets to train on, maybe translating the alpaca dataset to Sundanese and training on that.\\n\\nFor the comparisons, I understand that almost no one here understands Sundanese so it's best if you copy paste these to google translate to see what it's saying if you're interested.\\n\\n**Comparisons**\\n\\nWe can see that without the finetune the model just defaults to replying in Indonesian even though the character card is in Sundanese and says to only reply in Sundanese. This is probably because the model is trained on data that correlates that Sundanese is spoken in Indonesia where people speak Indonesian. It also definitely works best on answering random questions if you asked trivia questions that has a chance to be on Wikipedia.\\n\\nMistral-7B-v0.1:\\n\\n    AI\\n    Naon anu abdi tiasa ngabantosan anjeun?\\n    \\n    You\\n    pang jelaskeun naon balapan mobil formula 1 te\\n    \\n    AI\\n    Mobil Formula 1 adalah mobil yang digunakan dalam pertandingan mobil Formula 1. Mobil ini memiliki desain yang unik dan kuat, dan dioperasikan oleh pemimpin terbaik dari seluruh dunia. Pertandingan mobil Formula 1 menjadi salah satu kegiatan olahraga terpopuler di dunia, dan menarik banyak penonton.\\n    \\n    You\\n    Ai fernando alonso te saha? Pang terangkeun\\n    \\n    AI\\n    Fernando Alonso adalah seorang pemimpin mobil Formula 1 yang berasal dari Spanyol. Beliau telah menyegarkan piala mobil Formula 1 dua kali, dan telah menjadi salah satu pemimpin terbaik di dunia mobil Formula 1.\\n\\nMistral-7B-Sunda-v1.0\\n\\n    AI\\n    Naon anu abdi tiasa ngabantosan anjeun?\\n    \\n    You\\n    pang jelaskeun naon balapan mobil formula 1 te\\n    \\n    AI\\n    Formula 1 mangrupa salah sahiji kompetisi mobil anu diadegkeun taun 1946, anu dipingpin ku FÃ©dÃ©ration Internationale de l'Automobile (FIA). Kompetisi ieu dilaksanakeun unggal usum ti bulan Maret nepi ka NopÃ©mber. Balapan Formula 1 biasana dilakukeun di laporan anu panjangna kurang leuwih 5-7 km, anu mana mobil-mobil anu parigel dibawa ku tim-tim anu aya di dunia.\\n    \\n    You\\n    Ai fernando alonso te saha? Pang terangkeun\\n    \\n    AI\\n    Fernando Alonso nyaÃ©ta saorang pembalap asal Spanyol anu lahir tanggal 29 Juli 1981 di Oviedo, Asturias, Spanyol. Anjeuna mimiti karir profesionalna di taun 1999, nalika anjeuna meunang gelar juara GP2 Series taun 2000. Dina taun 2001, Alonso mimiti maÃ©n di Formula One, nalika anjeuna ditandatanganan kontrak ku Tim Minardi. Tapi, sabada dua kali balapan, anjeuna diganti ku Alexander Wurz. Taun 2003, Alonso ganti tim ka Tim Renault, anu mana anjeuna bisa nyitak rÃ©kor anyar minangka pembalap paling umurna anu bisa meunang podium di Grand Prix Monaco. Podium Ã©ta ogÃ© jadi hiji-hijina podium anu didamel ku Alonso di musim 2003.\\n\\nThe original Mistral 7B model also devolves to random rambling when conversed in Sundanese about things that are not Wikipedia trivia questions. Whereas the finetuned model actually replied coherently and mostly correctly.\\n\\nMistral-7B-v0.1:\\n\\n    AI\\n    Naon anu abdi tiasa ngabantosan anjeun?\\n    \\n    You\\n    kumaha carana masak endog goreng\\n    \\n    AI\\n    Kutu kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas,\\n\\nMistral-7B-Sunda-v1.0\\n\\n    AI\\n    Naon anu abdi tiasa ngabantosan anjeun?\\n    \\n    You\\n    kumaha carana masak endog goreng\\n    \\n    AI\\n    Endog goreng bisa dijieun ku cara mÃ©rÃ© endog kana minyak panas. Anda bisa menggunakan wajan atau pemangkok untuk memasaknya. Anda juga bisa menambahkan bumbu seperti bawang bombay, lada, dan uyah untuk menambah rasa.\\n\\n&amp;#x200B;\", 'author_fullname': 't2_17brr5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] I tried to teach Mistral 7B a new language (Sundanese) and it worked! (sort of)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18ocba4', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 93, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 93, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703242460.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Nero10578/Mistral-7B-Sunda-v1.0\"&gt;Nero10578/Mistral-7B-Sunda-v1.0 Â· Hugging Face&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll start by saying I am not a machine learning expert and I am new to this since getting into LLMs as it got popular since LLaMa release. So, I don&amp;#39;t know much of the technicalities although I am willing to learn.&lt;/p&gt;\\n\\n&lt;p&gt;Seeing that even Bing chat which is powered by chatGPT-4 couldn&amp;#39;t speak in Sundanese when asked, I thought of trying to teach Mistral-7B Sundanese using just QLora training. It surprisingly worked out pretty well for how little data I had to train it with.&lt;/p&gt;\\n\\n&lt;p&gt;Why Sundanese? Because I can speak it and it is a regional language in Indonesia that isn&amp;#39;t used much if at all on the internet so there was basically almost no chance it was trained well on any of these LLM models coming out.&lt;/p&gt;\\n\\n&lt;p&gt;This is more of an exercise to see if a small open-source model like Mistral 7B can be trained to learn a new very obscure language that is apparently never there in the original dataset. More details on the dataset and training settings are in the huggingface model card. Please ask me if you have any questions. Also, I have no idea how I am supposed to name the version, but I just called this 1.0 because the 0.x versions are basically me trial and erroring burning up electricity and heating my place for quite a while.&lt;/p&gt;\\n\\n&lt;p&gt;The resulting model is actually surprisingly coherent and translates knowledge in English to Sundanese decently. Although it is a bit unstable in that sometimes it replies with incorrect information or irrelevant information, which is probably because of the limited dataset. Next steps are going to be to get more datasets to train on, maybe translating the alpaca dataset to Sundanese and training on that.&lt;/p&gt;\\n\\n&lt;p&gt;For the comparisons, I understand that almost no one here understands Sundanese so it&amp;#39;s best if you copy paste these to google translate to see what it&amp;#39;s saying if you&amp;#39;re interested.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Comparisons&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;We can see that without the finetune the model just defaults to replying in Indonesian even though the character card is in Sundanese and says to only reply in Sundanese. This is probably because the model is trained on data that correlates that Sundanese is spoken in Indonesia where people speak Indonesian. It also definitely works best on answering random questions if you asked trivia questions that has a chance to be on Wikipedia.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral-7B-v0.1:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;AI\\nNaon anu abdi tiasa ngabantosan anjeun?\\n\\nYou\\npang jelaskeun naon balapan mobil formula 1 te\\n\\nAI\\nMobil Formula 1 adalah mobil yang digunakan dalam pertandingan mobil Formula 1. Mobil ini memiliki desain yang unik dan kuat, dan dioperasikan oleh pemimpin terbaik dari seluruh dunia. Pertandingan mobil Formula 1 menjadi salah satu kegiatan olahraga terpopuler di dunia, dan menarik banyak penonton.\\n\\nYou\\nAi fernando alonso te saha? Pang terangkeun\\n\\nAI\\nFernando Alonso adalah seorang pemimpin mobil Formula 1 yang berasal dari Spanyol. Beliau telah menyegarkan piala mobil Formula 1 dua kali, dan telah menjadi salah satu pemimpin terbaik di dunia mobil Formula 1.\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Mistral-7B-Sunda-v1.0&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;AI\\nNaon anu abdi tiasa ngabantosan anjeun?\\n\\nYou\\npang jelaskeun naon balapan mobil formula 1 te\\n\\nAI\\nFormula 1 mangrupa salah sahiji kompetisi mobil anu diadegkeun taun 1946, anu dipingpin ku FÃ©dÃ©ration Internationale de l&amp;#39;Automobile (FIA). Kompetisi ieu dilaksanakeun unggal usum ti bulan Maret nepi ka NopÃ©mber. Balapan Formula 1 biasana dilakukeun di laporan anu panjangna kurang leuwih 5-7 km, anu mana mobil-mobil anu parigel dibawa ku tim-tim anu aya di dunia.\\n\\nYou\\nAi fernando alonso te saha? Pang terangkeun\\n\\nAI\\nFernando Alonso nyaÃ©ta saorang pembalap asal Spanyol anu lahir tanggal 29 Juli 1981 di Oviedo, Asturias, Spanyol. Anjeuna mimiti karir profesionalna di taun 1999, nalika anjeuna meunang gelar juara GP2 Series taun 2000. Dina taun 2001, Alonso mimiti maÃ©n di Formula One, nalika anjeuna ditandatanganan kontrak ku Tim Minardi. Tapi, sabada dua kali balapan, anjeuna diganti ku Alexander Wurz. Taun 2003, Alonso ganti tim ka Tim Renault, anu mana anjeuna bisa nyitak rÃ©kor anyar minangka pembalap paling umurna anu bisa meunang podium di Grand Prix Monaco. Podium Ã©ta ogÃ© jadi hiji-hijina podium anu didamel ku Alonso di musim 2003.\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;The original Mistral 7B model also devolves to random rambling when conversed in Sundanese about things that are not Wikipedia trivia questions. Whereas the finetuned model actually replied coherently and mostly correctly.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral-7B-v0.1:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;AI\\nNaon anu abdi tiasa ngabantosan anjeun?\\n\\nYou\\nkumaha carana masak endog goreng\\n\\nAI\\nKutu kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Mistral-7B-Sunda-v1.0&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;AI\\nNaon anu abdi tiasa ngabantosan anjeun?\\n\\nYou\\nkumaha carana masak endog goreng\\n\\nAI\\nEndog goreng bisa dijieun ku cara mÃ©rÃ© endog kana minyak panas. Anda bisa menggunakan wajan atau pemangkok untuk memasaknya. Anda juga bisa menambahkan bumbu seperti bawang bombay, lada, dan uyah untuk menambah rasa.\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?auto=webp&amp;s=3eb0d5a3a87720969d76b11db53258e3e6a89a1b', 'width': 1200, 'height': 648}, 'resolutions': [{'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=24d2d9104f9d0e2fc4966eeb323ac2a1fb6cca96', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9c7deb8d35596a78e69538d30ef497805909495', 'width': 216, 'height': 116}, {'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c69332c9604ffd01ef659afd350f29c7df5ffc9c', 'width': 320, 'height': 172}, {'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69020f049040bce254750725b0964544831da80a', 'width': 640, 'height': 345}, {'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef3918a4863a473185d48e33bbbb9bed8b538dae', 'width': 960, 'height': 518}, {'url': 'https://external-preview.redd.it/FvkYiF4N1m6tNlmNT4G-9GCgIBclpimu1SFx7b_g-Ic.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b71abb8f387fe18d7be48e1727d3ef616ae8d022', 'width': 1080, 'height': 583}], 'variants': {}, 'id': 'Kbc4t8i3_BJ1yNpqg-shARbU8csjm5G1LtpVQYX83oQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ocba4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'nero10578', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/', 'subreddit_subscribers': 2843382, 'created_utc': 1703242460.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '\"The paper demonstrates the effectiveness of DeepPCR through various applications. It achieves speedups of up to 30Ã— for forward and 200Ã— for backward passes in multi-layer perceptrons. Additionally, the algorithm is applied to parallelize training of deep ResNet architectures and generation in diffusion models, resulting in up to 7Ã— faster training and 11Ã— faster generation.\"\\n\\nPaper: [https://arxiv.org/pdf/2309.16318.pdf](https://arxiv.org/pdf/2309.16318.pdf)\\n\\nResearch page: [https://machinelearning.apple.com/research/deeppcr](https://machinelearning.apple.com/research/deeppcr)\\n\\n[https://twitter.com/i/status/1735876638947348656](https://twitter.com/i/status/1735876638947348656)', 'author_fullname': 't2_10v86q', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[News] Apple Researchers Unveil DeepPCR: A Novel Machine Learning Algorithm that Parallelizes Typically Sequential Operations in Order to Speed Up Inference and Training of Neural Networks', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'two', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18odo0m', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 45, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'News', 'can_mod_post': False, 'score': 45, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703247573.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;The paper demonstrates the effectiveness of DeepPCR through various applications. It achieves speedups of up to 30Ã— for forward and 200Ã— for backward passes in multi-layer perceptrons. Additionally, the algorithm is applied to parallelize training of deep ResNet architectures and generation in diffusion models, resulting in up to 7Ã— faster training and 11Ã— faster generation.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/pdf/2309.16318.pdf\"&gt;https://arxiv.org/pdf/2309.16318.pdf&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Research page: &lt;a href=\"https://machinelearning.apple.com/research/deeppcr\"&gt;https://machinelearning.apple.com/research/deeppcr&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://twitter.com/i/status/1735876638947348656\"&gt;https://twitter.com/i/status/1735876638947348656&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18odo0m', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'paryska99', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18odo0m/news_apple_researchers_unveil_deeppcr_a_novel/', 'subreddit_subscribers': 2843382, 'created_utc': 1703247573.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '', 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Alignment Horseshoe', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_18ojxf8', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'author_fullname': 't2_l0o7s29o', 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/yoMcgOqMdjVtHEnoib3-EjMd2VcU9HG1R_f_t9PHmwE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'crosspost_parent_list': [{'approved_at_utc': None, 'subreddit': 'ChatGPT', 'selftext': '', 'author_fullname': 't2_15x23kkv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'is_gallery': True, 'title': 'ChatGPT cannot make darker skinned people ðŸ˜­ðŸ˜­ðŸ˜­', 'link_flair_richtext': [{'e': 'text', 't': 'Funny '}], 'subreddit_name_prefixed': 'r/ChatGPT', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'wi3p8tcrcq7c1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 216, 'x': 108, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e5fcb6130c90484b45a6a20d3742f92b376d0a4'}, {'y': 432, 'x': 216, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=863aa120e2e9a2a938c67791238518cba611d413'}, {'y': 640, 'x': 320, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae234e024699897ca54c57406b01ce55284fe270'}, {'y': 1280, 'x': 640, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9ab0f2e7d6370c522ab0bf00e7973919d4cab1a'}, {'y': 1920, 'x': 960, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=acb68a2221c0eccc094af55a11461012dbc677f4'}, {'y': 2160, 'x': 1080, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d1482772f492ca4df36caff1da2b9de6a18f052'}], 's': {'y': 2400, 'x': 1080, 'u': 'https://preview.redd.it/wi3p8tcrcq7c1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=9eebb826b8647171f6f3a3873e071eb68f5e37c5'}, 'id': 'wi3p8tcrcq7c1'}, '184gcgarcq7c1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 216, 'x': 108, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f71cb45820124ac62a936e2b41e46e32df7fd903'}, {'y': 432, 'x': 216, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=809f260eec8b81519af646c330e764f287e9c1a7'}, {'y': 640, 'x': 320, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9fad94831e77cbd35801d8e6ff631ad8a87b1c6'}, {'y': 1280, 'x': 640, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12a51373b0fa385f2a22a61000cbd0ff95676a50'}, {'y': 1920, 'x': 960, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=557a615f279d1c038d2e6b7f0b246a485e557d11'}, {'y': 2160, 'x': 1080, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37e1d51621ce667ef2fe0dc52ba819c6b50e3dd0'}], 's': {'y': 2400, 'x': 1080, 'u': 'https://preview.redd.it/184gcgarcq7c1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8a1592b7119fcb3780d378c63bd9d53666481048'}, 'id': '184gcgarcq7c1'}, '3mabu8ercq7c1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 216, 'x': 108, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0749ed1697f166cecdd6c8b864c17aa0f744c0d'}, {'y': 432, 'x': 216, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b7cbac06e5786455f7098c1c1a61c52f74893df'}, {'y': 640, 'x': 320, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d15380e3206d870bb615e7b8079d5ecd3e18000'}, {'y': 1280, 'x': 640, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=969ab3b86e8bc4cd4af6a6353165c31d27156ee5'}, {'y': 1920, 'x': 960, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a3d64d481799e2315d18cd598f5bf1a8a47e554'}, {'y': 2160, 'x': 1080, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=040df9859ec92a19d7d866eedc2d3da375e912aa'}], 's': {'y': 2400, 'x': 1080, 'u': 'https://preview.redd.it/3mabu8ercq7c1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=508658a4b79c7182ebf74195df4e38f318318441'}, 'id': '3mabu8ercq7c1'}}, 'name': 't3_18o0590', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'ups': 2853, 'domain': 'reddit.com', 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'gallery_data': {'items': [{'media_id': '184gcgarcq7c1', 'id': 377299917}, {'media_id': 'wi3p8tcrcq7c1', 'id': 377299918}, {'media_id': '3mabu8ercq7c1', 'id': 377299919}]}, 'link_flair_text': 'Funny ', 'can_mod_post': False, 'score': 2853, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/yoMcgOqMdjVtHEnoib3-EjMd2VcU9HG1R_f_t9PHmwE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1703200238.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'total_awards_received': 0, 'allow_live_comments': True, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/18o0590', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '935162a0-7be9-11ed-913e-6a257d69e3b3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_7hqomg', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0dd3bb', 'id': '18o0590', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Hayden-Boyer', 'discussion_type': None, 'num_comments': 321, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ChatGPT/comments/18o0590/chatgpt_cannot_make_darker_skinned_people/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/gallery/18o0590', 'subreddit_subscribers': 3857244, 'created_utc': 1703200238.0, 'num_crossposts': 3, 'media': None, 'is_video': False}], 'created': 1703265436.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'reddit.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/18o0590', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ojxf8', 'is_robot_indexable': True, 'report_reasons': None, 'author': '31162123', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'crosspost_parent': 't3_18o0590', 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ojxf8/d_alignment_horseshoe/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/gallery/18o0590', 'subreddit_subscribers': 2843382, 'created_utc': 1703265436.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Iâ€™ve been training all my models on the university servers for the past four years, however, as I graduate soon - that simply wonâ€™t do anymoreâ€¦\\n\\nIâ€™ve always been on Mac, but the thought of me not being able to run CUDA after graduating is tough hahah - thinking of setting up the cheapest possible computer just for training AI, however, I have no clue where to start when it comes to windows / Linux computers - and also donâ€™t really understand the difference between all the GPUs out there\\n\\nDo you guys know any cheap, but good computers for AI - if theyâ€™re possible to buy used, then even better!', 'author_fullname': 't2_p3uompls', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] What are some cheap and OK devices for training on CUDA?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18o99lr', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703229757.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Iâ€™ve been training all my models on the university servers for the past four years, however, as I graduate soon - that simply wonâ€™t do anymoreâ€¦&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™ve always been on Mac, but the thought of me not being able to run CUDA after graduating is tough hahah - thinking of setting up the cheapest possible computer just for training AI, however, I have no clue where to start when it comes to windows / Linux computers - and also donâ€™t really understand the difference between all the GPUs out there&lt;/p&gt;\\n\\n&lt;p&gt;Do you guys know any cheap, but good computers for AI - if theyâ€™re possible to buy used, then even better!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18o99lr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Middle_Stomach_6681', 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18o99lr/d_what_are_some_cheap_and_ok_devices_for_training/', 'subreddit_subscribers': 2843382, 'created_utc': 1703229757.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '**Paper**: [https://arxiv.org/abs/2312.06902](https://arxiv.org/abs/2312.06902)\\n\\n**Project page**: [https://ml.energy/zeus/perseus/](https://ml.energy/zeus/perseus/)\\n\\n**Integrating**: [https://ml.energy/zeus/perseus/integrating/](https://ml.energy/zeus/perseus/integrating/)\\n\\n**Abstract**:\\n\\n&gt;Training large AI models on numerous GPUs consumes a massive amount of  energy. We observe that not all energy consumed during training directly  contributes to end-to-end training throughput, and a significant  portion can be removed without slowing down training, which we call  *energy bloat*.   \\nIn this work, we identify two independent sources of energy bloat in  large model training, *intrinsic* and *extrinsic*, and propose **Perseus**, a  unified optimization framework that mitigates both. Perseus obtains the  \"iteration time-energy\" Pareto frontier of any large model training job  using an efficient iterative graph cut-based algorithm and schedules  energy consumption of its forward and backward computations across time  to remove intrinsic and extrinsic energy bloat. Evaluation on large  models like GPT-3 and Bloom shows that Perseus reduces energy  consumption of large model training by up to 30%, enabling savings  otherwise unobtainable before.', 'author_fullname': 't2_mveclxvsc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Perseus: Removing Energy Bloat from Large Model Training', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18oedd2', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703249984.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=\"https://arxiv.org/abs/2312.06902\"&gt;https://arxiv.org/abs/2312.06902&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Project page&lt;/strong&gt;: &lt;a href=\"https://ml.energy/zeus/perseus/\"&gt;https://ml.energy/zeus/perseus/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Integrating&lt;/strong&gt;: &lt;a href=\"https://ml.energy/zeus/perseus/integrating/\"&gt;https://ml.energy/zeus/perseus/integrating/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Training large AI models on numerous GPUs consumes a massive amount of  energy. We observe that not all energy consumed during training directly  contributes to end-to-end training throughput, and a significant  portion can be removed without slowing down training, which we call  &lt;em&gt;energy bloat&lt;/em&gt;.&lt;br/&gt;\\nIn this work, we identify two independent sources of energy bloat in  large model training, &lt;em&gt;intrinsic&lt;/em&gt; and &lt;em&gt;extrinsic&lt;/em&gt;, and propose &lt;strong&gt;Perseus&lt;/strong&gt;, a  unified optimization framework that mitigates both. Perseus obtains the  &amp;quot;iteration time-energy&amp;quot; Pareto frontier of any large model training job  using an efficient iterative graph cut-based algorithm and schedules  energy consumption of its forward and backward computations across time  to remove intrinsic and extrinsic energy bloat. Evaluation on large  models like GPT-3 and Bloom shows that Perseus reduces energy  consumption of large model training by up to 30%, enabling savings  otherwise unobtainable before.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18oedd2', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'APaperADay', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18oedd2/r_perseus_removing_energy_bloat_from_large_model/', 'subreddit_subscribers': 2843382, 'created_utc': 1703249984.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'I am working on a project that will take PDFs and translate them to code. I canâ€™t be too specific about the PDFS I\\'m using. But does anyone have any advice on what model I should use/finetune? I can create a large dataset from existing PDFs and existing Code. But I\\'m unsure of the \"how\" (model/training/finetuning etc.)\\n\\nAn ideal solution would be a model I could use at work so anything that I can program/train independently would be great.\\n\\nAny advice would be appreciated thanks!', 'author_fullname': 't2_2jgmtz96', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Training Local LLM to Translate Text into Code', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18oj5tq', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703263408.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project that will take PDFs and translate them to code. I canâ€™t be too specific about the PDFS I&amp;#39;m using. But does anyone have any advice on what model I should use/finetune? I can create a large dataset from existing PDFs and existing Code. But I&amp;#39;m unsure of the &amp;quot;how&amp;quot; (model/training/finetuning etc.)&lt;/p&gt;\\n\\n&lt;p&gt;An ideal solution would be a model I could use at work so anything that I can program/train independently would be great.&lt;/p&gt;\\n\\n&lt;p&gt;Any advice would be appreciated thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18oj5tq', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'slb1357', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18oj5tq/p_training_local_llm_to_translate_text_into_code/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18oj5tq/p_training_local_llm_to_translate_text_into_code/', 'subreddit_subscribers': 2843382, 'created_utc': 1703263408.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'I always seem to get conflicting answers on this, thoughts?', 'author_fullname': 't2_495cn7pm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D]When should and shouldnâ€™t you balance an unbalanced dataset?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18oct9r', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703244380.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I always seem to get conflicting answers on this, thoughts?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18oct9r', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Throwawayforgainz99', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18oct9r/dwhen_should_and_shouldnt_you_balance_an/', 'subreddit_subscribers': 2843382, 'created_utc': 1703244380.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '**Paper**: [https://openreview.net/forum?id=vKpVJxplmB](https://openreview.net/forum?id=vKpVJxplmB)\\n\\n**Code**: PyTorch implementation code included in Supplemental Materials.\\n\\n**Abstract**:\\n\\n&gt;The hippocampus plays a critical role in learning, memory, and spatial  representation, processes that depend on the NMDA receptor (NMDAR).  Inspired by recent findings that compare deep learning models to the  hippocampus, we propose a new nonlinear activation function that mimics  NMDAR dynamics. NMDAR-like nonlinearity has a beneficial role in  shifting short-term working memory into long-term reference memory in  transformers, thus enhancing a process that is similar to memory  consolidation in the mammalian brain. We design a navigation task  assessing these two memory functions and show that manipulating the  activation function (i.e., mimicking the Mg**^(2+)**\\\\-gating  of NMDAR) disrupts long-term memory processes. Our experiments suggest  that place cell-like functions and reference memory reside in the  feed-forward network layer of transformers and that nonlinearity drives  these processes. We discuss the role of NMDAR-like nonlinearity in  establishing this striking resemblance between transformer architecture  and hippocampal spatial representation.', 'author_fullname': 't2_mveclxvsc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18oellc', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703250682.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=\"https://openreview.net/forum?id=vKpVJxplmB\"&gt;https://openreview.net/forum?id=vKpVJxplmB&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: PyTorch implementation code included in Supplemental Materials.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The hippocampus plays a critical role in learning, memory, and spatial  representation, processes that depend on the NMDA receptor (NMDAR).  Inspired by recent findings that compare deep learning models to the  hippocampus, we propose a new nonlinear activation function that mimics  NMDAR dynamics. NMDAR-like nonlinearity has a beneficial role in  shifting short-term working memory into long-term reference memory in  transformers, thus enhancing a process that is similar to memory  consolidation in the mammalian brain. We design a navigation task  assessing these two memory functions and show that manipulating the  activation function (i.e., mimicking the Mg&lt;strong&gt;&lt;sup&gt;2+&lt;/sup&gt;&lt;/strong&gt;-gating  of NMDAR) disrupts long-term memory processes. Our experiments suggest  that place cell-like functions and reference memory reside in the  feed-forward network layer of transformers and that nonlinearity drives  these processes. We discuss the role of NMDAR-like nonlinearity in  establishing this striking resemblance between transformer architecture  and hippocampal spatial representation.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?auto=webp&amp;s=71ad6a8a2e6e5fac511957278effb619d3b30998', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c811689cb2c2b238253833845bad24e74bdb5d8', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79517bf9d18cf488552e43744ad2c342af22479f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4b56b82708f12907eed5cb9688415ff2947f8a5', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'A2cFENtZsGUk4TdgVLLL25zXBQBwmcPSG87hZLopV-w'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18oellc', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'APaperADay', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18oellc/r_transformer_as_a_hippocampal_memory/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18oellc/r_transformer_as_a_hippocampal_memory/', 'subreddit_subscribers': 2843382, 'created_utc': 1703250682.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'When I ask chatGPT for example about giving me the number of letters in this sentence:\\n\\nBut, I am wondering how these LLMs can perform the count, knowing that tokenizers are doing subtoken level and not character level (which means each subtoken is \"maybe\" not aware of the characters it has\").).\\n\\n  \\nThe answer is 15 which is correct\\n\\n&amp;#x200B;\\n\\nBut, I am wondering how can these LLMs perform the count, knowing that tokenizers are doing subtoken level and not character level (means each subtoken is \"maybe\" not aware about the characters it have\").\\n\\n&amp;#x200B;\\n\\n&amp;#x200B;', 'author_fullname': 't2_e0a821g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] How can LLMs be aware of the characters existing within each subtoken?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18odz8f', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.64, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703248658.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I ask chatGPT for example about giving me the number of letters in this sentence:&lt;/p&gt;\\n\\n&lt;p&gt;But, I am wondering how these LLMs can perform the count, knowing that tokenizers are doing subtoken level and not character level (which means each subtoken is &amp;quot;maybe&amp;quot; not aware of the characters it has&amp;quot;).).&lt;/p&gt;\\n\\n&lt;p&gt;The answer is 15 which is correct&lt;/p&gt;\\n\\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\\n\\n&lt;p&gt;But, I am wondering how can these LLMs perform the count, knowing that tokenizers are doing subtoken level and not character level (means each subtoken is &amp;quot;maybe&amp;quot; not aware about the characters it have&amp;quot;).&lt;/p&gt;\\n\\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\\n\\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18odz8f', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'kekkimo', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18odz8f/d_how_can_llms_be_aware_of_the_characters/', 'subreddit_subscribers': 2843382, 'created_utc': 1703248658.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'After all the hubbub around the MMLU (for example [my article](https://derenrich.medium.com/errors-in-the-mmlu-the-deep-learning-benchmark-is-wrong-surprisingly-often-7258bb045859)) I thought I would make an interface for seeing how humans do versus even middle of the pack LLM. It\\'s called [Are You Smarter Than An LLM](https://d.erenrich.net/are-you-smarter-than-an-llm/index.html)?\\n\\nIt presents you random questions from the MMLU and compares your answers to the LLM. Click the \"what is this\" button at the bottom for more details on how it works.\\n\\nFeedback appreciated!', 'author_fullname': 't2_pm8ge', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Deep dive into the MMLU (\"Are you smarter than an LLM?\")', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18ntia7', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 83, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 83, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703182888.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After all the hubbub around the MMLU (for example &lt;a href=\"https://derenrich.medium.com/errors-in-the-mmlu-the-deep-learning-benchmark-is-wrong-surprisingly-often-7258bb045859\"&gt;my article&lt;/a&gt;) I thought I would make an interface for seeing how humans do versus even middle of the pack LLM. It&amp;#39;s called &lt;a href=\"https://d.erenrich.net/are-you-smarter-than-an-llm/index.html\"&gt;Are You Smarter Than An LLM&lt;/a&gt;?&lt;/p&gt;\\n\\n&lt;p&gt;It presents you random questions from the MMLU and compares your answers to the LLM. Click the &amp;quot;what is this&amp;quot; button at the bottom for more details on how it works.&lt;/p&gt;\\n\\n&lt;p&gt;Feedback appreciated!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ntia7', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'brokensegue', 'discussion_type': None, 'num_comments': 46, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ntia7/d_deep_dive_into_the_mmlu_are_you_smarter_than_an/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ntia7/d_deep_dive_into_the_mmlu_are_you_smarter_than_an/', 'subreddit_subscribers': 2843382, 'created_utc': 1703182888.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': \"I'm training a model for captioning that uses a Perceiver Resampler, like the one in Flamingo, the only difference being that I don't concatenate the learnt query parameters to the keys and queries for cross-attention.  \\n\\n\\nEverything was going well up until I reached roughly 50% of my dataset (first epoch) and this happened. I am also testing other architectures, like not using the Perceiver Resampler at all and just using all the features I get from my vision encoders, and I get something like this at around 50% of my dataset too but not at the exact same iteration (I'm using a seed so it should be the exact same on if it's a data issue, right?). Other architectures have reached the 8000 iteration mark and did not have any kind of increase in the training loss like this... why could this be happening?\\n\\n[Training Loss at \\\\~50&amp;#37; of my first epoch ](https://preview.redd.it/p3nx6xk96u7c1.png?width=543&amp;format=png&amp;auto=webp&amp;s=531991c8615ce1b04aa28c47fe48a48411ec1231)\", 'author_fullname': 't2_mbc0bpphd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Increase in training loss while in the first epoch', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': 106, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'p3nx6xk96u7c1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 82, 'x': 108, 'u': 'https://preview.redd.it/p3nx6xk96u7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc2a40b4353b7234d418dffba9edc9a3f489932b'}, {'y': 164, 'x': 216, 'u': 'https://preview.redd.it/p3nx6xk96u7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6d38db7c7dce39f04d4c7c77032514d039631c6'}, {'y': 243, 'x': 320, 'u': 'https://preview.redd.it/p3nx6xk96u7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfb952117b722de1a26864ce8ccec2365ff2a37f'}], 's': {'y': 413, 'x': 543, 'u': 'https://preview.redd.it/p3nx6xk96u7c1.png?width=543&amp;format=png&amp;auto=webp&amp;s=531991c8615ce1b04aa28c47fe48a48411ec1231'}, 'id': 'p3nx6xk96u7c1'}}, 'name': 't3_18odi68', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/N2I1GpsKoR9ZD-1ZSMC9nv06SouuRKFmfYx01KkRP8M.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703246994.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m training a model for captioning that uses a Perceiver Resampler, like the one in Flamingo, the only difference being that I don&amp;#39;t concatenate the learnt query parameters to the keys and queries for cross-attention.  &lt;/p&gt;\\n\\n&lt;p&gt;Everything was going well up until I reached roughly 50% of my dataset (first epoch) and this happened. I am also testing other architectures, like not using the Perceiver Resampler at all and just using all the features I get from my vision encoders, and I get something like this at around 50% of my dataset too but not at the exact same iteration (I&amp;#39;m using a seed so it should be the exact same on if it&amp;#39;s a data issue, right?). Other architectures have reached the 8000 iteration mark and did not have any kind of increase in the training loss like this... why could this be happening?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/p3nx6xk96u7c1.png?width=543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=531991c8615ce1b04aa28c47fe48a48411ec1231\"&gt;Training Loss at ~50&amp;#37; of my first epoch &lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18odi68', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'AromaticCantaloupe19', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18odi68/d_increase_in_training_loss_while_in_the_first/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18odi68/d_increase_in_training_loss_while_in_the_first/', 'subreddit_subscribers': 2843382, 'created_utc': 1703246994.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': \"I was given this assignment but however i have access to my own pc only. I dont think it has any gpu. I'm new to LLMs. Please guide me how to proceed.\\n\\nit is also mentioned that i can use any other model as well from Hugging Face.. thanks in advance\", 'author_fullname': 't2_a4tygchn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Fine-Tuning and Evaluating a Falcon 7B/LLAMA 7B Model for HTML Code Generation', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_18okus7', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703267912.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was given this assignment but however i have access to my own pc only. I dont think it has any gpu. I&amp;#39;m new to LLMs. Please guide me how to proceed.&lt;/p&gt;\\n\\n&lt;p&gt;it is also mentioned that i can use any other model as well from Hugging Face.. thanks in advance&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18okus7', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'CommunicationHot6434', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18okus7/p_finetuning_and_evaluating_a_falcon_7bllama_7b/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18okus7/p_finetuning_and_evaluating_a_falcon_7bllama_7b/', 'subreddit_subscribers': 2843382, 'created_utc': 1703267912.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ' Hi all , I would like to showcase a simple telegram bot that I made which converts text to images using Stable Diffusion.\\n\\nThe minimum requirements would be 6gb of VRAM.\\n\\nSadly,   right now python telegram bot only limits sending photos of up to 5mb,   hence the poor quality of images though I am finding a workaround for   it.  Any inputs would be valuable! :)\\n\\nHere is the link to the github:  \\n[https://github.com/harvestingmoon/StableVisionBot](https://github.com/harvestingmoon/StableVisionBot)', 'author_fullname': 't2_2rbl6du7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Stable Diffusion Telegram Bot', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_18ojyja', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703265522.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all , I would like to showcase a simple telegram bot that I made which converts text to images using Stable Diffusion.&lt;/p&gt;\\n\\n&lt;p&gt;The minimum requirements would be 6gb of VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Sadly,   right now python telegram bot only limits sending photos of up to 5mb,   hence the poor quality of images though I am finding a workaround for   it.  Any inputs would be valuable! :)&lt;/p&gt;\\n\\n&lt;p&gt;Here is the link to the github:&lt;br/&gt;\\n&lt;a href=\"https://github.com/harvestingmoon/StableVisionBot\"&gt;https://github.com/harvestingmoon/StableVisionBot&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?auto=webp&amp;s=9bce39f928a6d4b20df7ecf1dce920a79134960b', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=853c88fe9117f045b9073471e6af232431d60310', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e688b5ea59da93495fd9eb920dc6835c9769c020', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a37ab4b41e8944cce3ea02093f7888d40c1a92b', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0940c0c249bb513620cbe4b48e89ef93788d094', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=453ceccf4ba1ca9954ff20d47dab050ad1b637b4', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/e7oXPIjTXb033eFg0RI82oiEcuRNNRE7OUozKXz-6og.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=adab8d1eef110212d883f0eee9fbf1aabff0c5c2', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'LFwnHcA0Zx6TK-4oFVBhbazkmLOXOByK5IXkix0YH9I'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ojyja', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'notrealDirect', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ojyja/p_stable_diffusion_telegram_bot/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ojyja/p_stable_diffusion_telegram_bot/', 'subreddit_subscribers': 2843382, 'created_utc': 1703265522.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Can we build an interactive voice robot toy using openAI API or any other platform API?\\nOr\\nHow can we build it any other possible way?', 'author_fullname': 't2_uhxfu9gb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] interactive voice robot?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18odkqm', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703247248.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can we build an interactive voice robot toy using openAI API or any other platform API?\\nOr\\nHow can we build it any other possible way?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18odkqm', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'richierich1008', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18odkqm/d_interactive_voice_robot/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18odkqm/d_interactive_voice_robot/', 'subreddit_subscribers': 2843382, 'created_utc': 1703247248.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': \"Looking for those who are knowledgeable in weather stations. How to detect faulty weather sensors based on their readings alone?\\n\\nI'm doing a project regarding detection and prediction of faulty sensors employed at weather stations to improve maintenance and optimize scheduling. \\n\\nI've seen studies using operational data such as battery level, communication status, and temperature to detect faults. This is especially the case for industrial machines. They employ sensors to read the temperature, vibration, rotation, etc. of those machines. \\n\\nWhat method can I do if such data isn't available for weather stations? I also don't have maintenance data due to confidential concerns from our local weather agency. I only have access to historical weather data, which contains values of actual physical phenomenon being sensed (temperature, humidity, etc.) \\n\\nAnomaly detection would work but I'm thinking if it's really that accurate since sudden rain affects temperature, humidity, wind speed values confuse the model as saying the sensor's faulty. \\n\\nPardon me if I sound noob because this is only my second machine learning project.\", 'author_fullname': 't2_np7kv7lmp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Methods for Fault Detection of Sensors', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18ohvn5', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703260013.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for those who are knowledgeable in weather stations. How to detect faulty weather sensors based on their readings alone?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m doing a project regarding detection and prediction of faulty sensors employed at weather stations to improve maintenance and optimize scheduling. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve seen studies using operational data such as battery level, communication status, and temperature to detect faults. This is especially the case for industrial machines. They employ sensors to read the temperature, vibration, rotation, etc. of those machines. &lt;/p&gt;\\n\\n&lt;p&gt;What method can I do if such data isn&amp;#39;t available for weather stations? I also don&amp;#39;t have maintenance data due to confidential concerns from our local weather agency. I only have access to historical weather data, which contains values of actual physical phenomenon being sensed (temperature, humidity, etc.) &lt;/p&gt;\\n\\n&lt;p&gt;Anomaly detection would work but I&amp;#39;m thinking if it&amp;#39;s really that accurate since sudden rain affects temperature, humidity, wind speed values confuse the model as saying the sensor&amp;#39;s faulty. &lt;/p&gt;\\n\\n&lt;p&gt;Pardon me if I sound noob because this is only my second machine learning project.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ohvn5', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Funny_Shoe1772', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ohvn5/r_methods_for_fault_detection_of_sensors/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ohvn5/r_methods_for_fault_detection_of_sensors/', 'subreddit_subscribers': 2843382, 'created_utc': 1703260013.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'I have a project which uses factorization machine (based on [lightFM](https://github.com/lyst/lightfm)) for generating product recommendations. Since I am dealing with big data, I use spark for efficient data manipulations. Since LightFM is not related to Spark at all, I have to convert spark dataframes to numpy before feeding them to LightFM, which is very time consuming. For this reason, I am thinking about switching to Spark FM, but noticed that `spark.ml.recommendation` does not include FM model, instead Spark has a `FMClassifier` and `FMRegressor` for classification and regression tasks.\\n\\nI was wondering if it is possible to use any of those two models for building a recommendation system, has anyone else had a similar experience? Is it worth it to make a switch or should I just stick with LightFM?', 'author_fullname': 't2_1h42m1ix', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Using SparkFM for recommendation system', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18ofzta', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703254827.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a project which uses factorization machine (based on &lt;a href=\"https://github.com/lyst/lightfm\"&gt;lightFM&lt;/a&gt;) for generating product recommendations. Since I am dealing with big data, I use spark for efficient data manipulations. Since LightFM is not related to Spark at all, I have to convert spark dataframes to numpy before feeding them to LightFM, which is very time consuming. For this reason, I am thinking about switching to Spark FM, but noticed that &lt;code&gt;spark.ml.recommendation&lt;/code&gt; does not include FM model, instead Spark has a &lt;code&gt;FMClassifier&lt;/code&gt; and &lt;code&gt;FMRegressor&lt;/code&gt; for classification and regression tasks.&lt;/p&gt;\\n\\n&lt;p&gt;I was wondering if it is possible to use any of those two models for building a recommendation system, has anyone else had a similar experience? Is it worth it to make a switch or should I just stick with LightFM?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?auto=webp&amp;s=efae80d4e39b16f5a964180c104539601c4577f3', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=32f197cc4f470bcc4cceade1fb6f4a6728b2baa5', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba93cd943ef96aa05f35ab779063de09f6719523', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67f9f48e2ea0489be07ff70d409fd158032beb29', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b49014229b8e268237c96dce27c7abac12b8196', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c2f692285ca80626b1e7dec95ea0687983c27a2', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/_9w4cE3KuNzIHzM5aAEe4MSygUvx9ym81uHSy_RAp1M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a82ed176f16181b2b317890fd63b8b05b913064', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'tLK1D-y4ALRft_pfwwMlAAwdLtigYEVo-p25Z65uuig'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ofzta', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Modruc', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ofzta/d_using_sparkfm_for_recommendation_system/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ofzta/d_using_sparkfm_for_recommendation_system/', 'subreddit_subscribers': 2843382, 'created_utc': 1703254827.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ' I am currently working on a project which requires me to implement a real time face recognition software on an edge platform such as raspberry pi. We are currently trying to test multiple face recognition models on Windows to test accuracy. The project requires that the accuracy is as good as possible, 100% if possible.  \\nWhich models would you recommend for this use case, that is real time Least demanding with highest accuracy.  \\nCan you also recommend some resources for the same. ', 'author_fullname': 't2_4f0x63kb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Current State of the Art Face Recognition Model? [D]', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18oevls', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703251571.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on a project which requires me to implement a real time face recognition software on an edge platform such as raspberry pi. We are currently trying to test multiple face recognition models on Windows to test accuracy. The project requires that the accuracy is as good as possible, 100% if possible.&lt;br/&gt;\\nWhich models would you recommend for this use case, that is real time Least demanding with highest accuracy.&lt;br/&gt;\\nCan you also recommend some resources for the same. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18oevls', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Shaurya_Saxena', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18oevls/current_state_of_the_art_face_recognition_model_d/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18oevls/current_state_of_the_art_face_recognition_model_d/', 'subreddit_subscribers': 2843382, 'created_utc': 1703251571.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '\\n\\nSuppose i want to train a cat only classifier, that outputs a high probability when given an image of a cat and low or zero probability when given anything else. Now I can collect a few hundred cat images as positive class, but my negative class samples are practically infinite. How do i go about training such a classifier?  What is the official name of such training paradigm? please suggest which directions should i explore.', 'author_fullname': 't2_4c79bqbe', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D]How to train a binary classifier with infinite examples of the negative class', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18ob5az', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.56, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703237771.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose i want to train a cat only classifier, that outputs a high probability when given an image of a cat and low or zero probability when given anything else. Now I can collect a few hundred cat images as positive class, but my negative class samples are practically infinite. How do i go about training such a classifier?  What is the official name of such training paradigm? please suggest which directions should i explore.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ob5az', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'dopekid22', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18ob5az/dhow_to_train_a_binary_classifier_with_infinite/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ob5az/dhow_to_train_a_binary_classifier_with_infinite/', 'subreddit_subscribers': 2843382, 'created_utc': 1703237771.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': \"Hi everyone, I'm fairly new to the world of ML. It's been about 5 months since I started and I felt like I was making progress in the beginning but now I feel kind of lost. I also feel like I'm learning very slowly. Like sometimes I get stuck with a problem for hours only to realise later the solution was simple. I don't know if this is the case with everyone but I don't feel satisfied with the progress I've made. How much should I have progressed in 5 months? \\n\\nThere is alot I don't know but I don't even know what I don't know\\n\\nSo a roadmap of ML/ of your own journey would be appreciated... \\n\\nCurrently I'm learning sentiment analysis and focusing on NLP atm\", 'author_fullname': 't2_8h09u35i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Advice for Learning', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18ocwib', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'c5cf3b2a-6abd-11ea-a37b-0ebd427f43f1', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703244723.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m fairly new to the world of ML. It&amp;#39;s been about 5 months since I started and I felt like I was making progress in the beginning but now I feel kind of lost. I also feel like I&amp;#39;m learning very slowly. Like sometimes I get stuck with a problem for hours only to realise later the solution was simple. I don&amp;#39;t know if this is the case with everyone but I don&amp;#39;t feel satisfied with the progress I&amp;#39;ve made. How much should I have progressed in 5 months? &lt;/p&gt;\\n\\n&lt;p&gt;There is alot I don&amp;#39;t know but I don&amp;#39;t even know what I don&amp;#39;t know&lt;/p&gt;\\n\\n&lt;p&gt;So a roadmap of ML/ of your own journey would be appreciated... &lt;/p&gt;\\n\\n&lt;p&gt;Currently I&amp;#39;m learning sentiment analysis and focusing on NLP atm&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Student', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18ocwib', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Ok-Neighborhood-7690', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/MachineLearning/comments/18ocwib/d_advice_for_learning/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18ocwib/d_advice_for_learning/', 'subreddit_subscribers': 2843382, 'created_utc': 1703244723.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'I am an attorney who, for better or worse, spends 80% of his day drafting and sending emails and some documents. (Who am I kidding, itâ€™s really kind of the worst!) I also keep track of every six minutes of my day (0.1 hour increments) with a description of what I was doing during that time. For example, if I worked on Matter A for 18 minutes a description may look something like the following: \\n\\n0.3 hr    Matter A    Reviewed complaint and outlined affirmative defenses to breach of contract claim; email to client regarding the same \\n\\nTimekeeping is easily the most tedious part of the job. \\n\\nTwo questions:  \\n\\n-Is it possible to train an LLM to review all my emails from a day, and suggest time entries for the various matters I worked on using their content and time stamps? \\n\\n-If it possible, where do I even start? \\n\\n-Oh, and confidentiality is key. (I guess there are three questions.)', 'author_fullname': 't2_3udwh53u', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Where do I begin for timekeeping?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18oi8v8', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703260982.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an attorney who, for better or worse, spends 80% of his day drafting and sending emails and some documents. (Who am I kidding, itâ€™s really kind of the worst!) I also keep track of every six minutes of my day (0.1 hour increments) with a description of what I was doing during that time. For example, if I worked on Matter A for 18 minutes a description may look something like the following: &lt;/p&gt;\\n\\n&lt;p&gt;0.3 hr    Matter A    Reviewed complaint and outlined affirmative defenses to breach of contract claim; email to client regarding the same &lt;/p&gt;\\n\\n&lt;p&gt;Timekeeping is easily the most tedious part of the job. &lt;/p&gt;\\n\\n&lt;p&gt;Two questions:  &lt;/p&gt;\\n\\n&lt;p&gt;-Is it possible to train an LLM to review all my emails from a day, and suggest time entries for the various matters I worked on using their content and time stamps? &lt;/p&gt;\\n\\n&lt;p&gt;-If it possible, where do I even start? &lt;/p&gt;\\n\\n&lt;p&gt;-Oh, and confidentiality is key. (I guess there are three questions.)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18oi8v8', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Psychological-Ad5390', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18oi8v8/p_where_do_i_begin_for_timekeeping/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18oi8v8/p_where_do_i_begin_for_timekeeping/', 'subreddit_subscribers': 2843382, 'created_utc': 1703260982.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Was curious about this coding question that I got in last yearâ€™s Meta AI Residency coding round (and got rejected after). The question was something on the lines of- code a convolutional neural network from scratch, using numpy and matrices. \\n\\nI was super startled and confused as most of my peers got LC Med questions, and I expected something like that as well (esp cause I didnâ€™t ever mention CNNs in my resume either).\\n\\nBut anyway, was curious if someone had a similar experience/would know the answer?\\n\\nThanks!\\n\\nEdit: For those who think this is a super basic question for an AI Residency interview, Iâ€™m happy for you, and I hope to be as well versed for it to be basic for me as well one day. But I just want to point out Meta AI had a workshop before the coding round to prepare us for it and covered what topics we should prepare for and they said LC Med-High questions (even mentioned topics to prepare such as Linked lists, Binary search trees, etc) and thatâ€™s what I was mentally prepared for.', 'author_fullname': 't2_d78s0gv9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Meta AI Residency Interview Question [D]', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18nio9k', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 62, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 62, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1703186381.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703149108.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was curious about this coding question that I got in last yearâ€™s Meta AI Residency coding round (and got rejected after). The question was something on the lines of- code a convolutional neural network from scratch, using numpy and matrices. &lt;/p&gt;\\n\\n&lt;p&gt;I was super startled and confused as most of my peers got LC Med questions, and I expected something like that as well (esp cause I didnâ€™t ever mention CNNs in my resume either).&lt;/p&gt;\\n\\n&lt;p&gt;But anyway, was curious if someone had a similar experience/would know the answer?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n\\n&lt;p&gt;Edit: For those who think this is a super basic question for an AI Residency interview, Iâ€™m happy for you, and I hope to be as well versed for it to be basic for me as well one day. But I just want to point out Meta AI had a workshop before the coding round to prepare us for it and covered what topics we should prepare for and they said LC Med-High questions (even mentioned topics to prepare such as Linked lists, Binary search trees, etc) and thatâ€™s what I was mentally prepared for.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18nio9k', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Immediate-Tailor-275', 'discussion_type': None, 'num_comments': 41, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18nio9k/meta_ai_residency_interview_question_d/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18nio9k/meta_ai_residency_interview_question_d/', 'subreddit_subscribers': 2843382, 'created_utc': 1703149108.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'I\\'m a hobbyist ML researcher and finally, after a year of work, built a state of the art machine vision model from scratch. It\\'s ViT-B/16 based, 448x448x3 input, 91M parameters, trained for 660M samples, with multi-label classification as the target task, on over 5000 unique tags.\\n\\nAll the big foundation vision models today were trained on heavily filtered datasets, greatly limiting the concepts they can represent, in line with arbitrary sets of rules for what is deemed \"wholesome\" by leading tech companies.  Everything from innocuous to spicy is on the chopping block of those filters.  And because CLIP pervades the industry, from StableDiffusion to LLaVA, so does OpenAI\\'s sensibilities.\\n\\nMy goal was to build a vision model for tagging images, mainly for labelling images for SD finetunes, but which wasn\\'t as heavily filtered and handicapped as CLIP/BLIP/LLaVA.  Something more inclusive, diverse, and sex positive.\\n\\nStarting from the wonderful work of SmilingWolf (https://github.com/SmilingWolf/SW-CV-ModelZoo) and the Danbooru2021 dataset, I iterated for a year on the model, training, and manually labeling a thousand images to help the model generalize beyond the danbooru domain.\\n\\nI\\'m releasing the first version of this model, dubbed JoyTag, today: https://github.com/fpgaminer/joytag\\n\\nIt achieves a mean F1 score of 0.578 across all of its over 5000 tags and across both the anime/manga styled images of the original danbooru dataset, but also photographs and other mediums thanks to the auxiliary training data I provided to it.\\n\\nIt was quite the struggle getting to this point, and I probably spent more time and money than any sane person should have.  I learned a lot about dealing with datasets as large as danbooru2021, training models at scale, and how to keep yourself awake all night so your 8xA100 rental doesn\\'t crash and blow all your money.\\n\\nIn my manual testing outside of even the validation set, the model has generalized well to unseen images, so I\\'m quite happy with the results thus far.  There\\'s plenty more work to do expanding its dataset to improve that F1 score further, and roundout its weak points.  With inclusivity and diversity being a major goal of this project, I\\'m disappointed by some of its remaining limitations (as documented in the GitHub README).  But I\\'m already busy manually tagging more images using my model-augmented workflow.\\n\\nI\\'m happy to answer questions about the project, the training procedure, anything.  All the training parameters are documented on GitHub, but there are so many little details that were hard won over the year.  Like that damned loss multiplier.  Ugh.\\n\\nGithub: https://github.com/fpgaminer/joytag\\nModel download: https://huggingface.co/fancyfeast/joytag/tree/main\\nDemo: https://huggingface.co/spaces/fancyfeast/joytag', 'author_fullname': 't2_bkfa9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': \"[P] I built an open SotA image tagging model to do what CLIP won't\", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18nb15l', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 201, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 201, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703122479.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a hobbyist ML researcher and finally, after a year of work, built a state of the art machine vision model from scratch. It&amp;#39;s ViT-B/16 based, 448x448x3 input, 91M parameters, trained for 660M samples, with multi-label classification as the target task, on over 5000 unique tags.&lt;/p&gt;\\n\\n&lt;p&gt;All the big foundation vision models today were trained on heavily filtered datasets, greatly limiting the concepts they can represent, in line with arbitrary sets of rules for what is deemed &amp;quot;wholesome&amp;quot; by leading tech companies.  Everything from innocuous to spicy is on the chopping block of those filters.  And because CLIP pervades the industry, from StableDiffusion to LLaVA, so does OpenAI&amp;#39;s sensibilities.&lt;/p&gt;\\n\\n&lt;p&gt;My goal was to build a vision model for tagging images, mainly for labelling images for SD finetunes, but which wasn&amp;#39;t as heavily filtered and handicapped as CLIP/BLIP/LLaVA.  Something more inclusive, diverse, and sex positive.&lt;/p&gt;\\n\\n&lt;p&gt;Starting from the wonderful work of SmilingWolf (&lt;a href=\"https://github.com/SmilingWolf/SW-CV-ModelZoo\"&gt;https://github.com/SmilingWolf/SW-CV-ModelZoo&lt;/a&gt;) and the Danbooru2021 dataset, I iterated for a year on the model, training, and manually labeling a thousand images to help the model generalize beyond the danbooru domain.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m releasing the first version of this model, dubbed JoyTag, today: &lt;a href=\"https://github.com/fpgaminer/joytag\"&gt;https://github.com/fpgaminer/joytag&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It achieves a mean F1 score of 0.578 across all of its over 5000 tags and across both the anime/manga styled images of the original danbooru dataset, but also photographs and other mediums thanks to the auxiliary training data I provided to it.&lt;/p&gt;\\n\\n&lt;p&gt;It was quite the struggle getting to this point, and I probably spent more time and money than any sane person should have.  I learned a lot about dealing with datasets as large as danbooru2021, training models at scale, and how to keep yourself awake all night so your 8xA100 rental doesn&amp;#39;t crash and blow all your money.&lt;/p&gt;\\n\\n&lt;p&gt;In my manual testing outside of even the validation set, the model has generalized well to unseen images, so I&amp;#39;m quite happy with the results thus far.  There&amp;#39;s plenty more work to do expanding its dataset to improve that F1 score further, and roundout its weak points.  With inclusivity and diversity being a major goal of this project, I&amp;#39;m disappointed by some of its remaining limitations (as documented in the GitHub README).  But I&amp;#39;m already busy manually tagging more images using my model-augmented workflow.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m happy to answer questions about the project, the training procedure, anything.  All the training parameters are documented on GitHub, but there are so many little details that were hard won over the year.  Like that damned loss multiplier.  Ugh.&lt;/p&gt;\\n\\n&lt;p&gt;Github: &lt;a href=\"https://github.com/fpgaminer/joytag\"&gt;https://github.com/fpgaminer/joytag&lt;/a&gt;\\nModel download: &lt;a href=\"https://huggingface.co/fancyfeast/joytag/tree/main\"&gt;https://huggingface.co/fancyfeast/joytag/tree/main&lt;/a&gt;\\nDemo: &lt;a href=\"https://huggingface.co/spaces/fancyfeast/joytag\"&gt;https://huggingface.co/spaces/fancyfeast/joytag&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?auto=webp&amp;s=228cf2ca80fb2b37a4842d98bbcbfcec4aba4de1', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b62660ac17c4c93457894f8d122b4ed361c089b3', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01a5be439ea7f93edee5419fdc68f3cb711d9a4e', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89a74b5e1b18e047b6ebf98b16784f3adeffbdb9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b119e6c1e113b88cb8db33c8e89b28fda16fe27e', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5493b4820f63626dd839d3d3d0b12fbaf790698', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/5gsQ9I_Rs5RkDMJ3_6apnKTs4T9mdy9pKeZJy7NOJ_A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c10300e7dfbdb03da3d782d246114c0cde9e467', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Ztxf6vcQJWzgHPe62AqjAdUUPYqYLaIn-8aC_3U1H3M'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18nb15l', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'fpgaminer', 'discussion_type': None, 'num_comments': 63, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18nb15l/p_i_built_an_open_sota_image_tagging_model_to_do/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18nb15l/p_i_built_an_open_sota_image_tagging_model_to_do/', 'subreddit_subscribers': 2843382, 'created_utc': 1703122479.0, 'num_crossposts': 2, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'The decimator is a function that removes points in the plot while keeping all the \"value/information\" of a chart. The post features examples with times series and clustering.\\n\\n[https://www.taipy.io/posts/big-data-charting-strategies-in-python](https://www.taipy.io/posts/big-data-charting-strategies-in-python)', 'author_fullname': 't2_tfe7ylgn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] the Decimator, or how to plot a lot of points', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18nq5p6', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.68, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703174094.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The decimator is a function that removes points in the plot while keeping all the &amp;quot;value/information&amp;quot; of a chart. The post features examples with times series and clustering.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.taipy.io/posts/big-data-charting-strategies-in-python\"&gt;https://www.taipy.io/posts/big-data-charting-strategies-in-python&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18nq5p6', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'quicklyalienated76', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18nq5p6/p_the_decimator_or_how_to_plot_a_lot_of_points/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18nq5p6/p_the_decimator_or_how_to_plot_a_lot_of_points/', 'subreddit_subscribers': 2843382, 'created_utc': 1703174094.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Hi\\n\\nI am planning to use Colab, but coding in colab (in folders format) is pain in the ass. I am planning to have Pycharm locally, and then automatically deploy (sync) with the google drive. How diffiuclt is this?', 'author_fullname': 't2_tdaoi81o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Pycharm with Google drive (for ML project)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18o89yr', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703225831.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\\n\\n&lt;p&gt;I am planning to use Colab, but coding in colab (in folders format) is pain in the ass. I am planning to have Pycharm locally, and then automatically deploy (sync) with the google drive. How diffiuclt is this?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18o89yr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Even_Campaign7385', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/18o89yr/d_pycharm_with_google_drive_for_ml_project/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18o89yr/d_pycharm_with_google_drive_for_ml_project/', 'subreddit_subscribers': 2843382, 'created_utc': 1703225831.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}, {'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Disclaimer: I am very much an amateur when in comes to ML.\\n\\nI am currently working on building CNN based classification models (python/pytorch) using large imaging files (digitized histology slides to be exact). Each file is at least 1 gb and I am training the models on thousands of files (about 4 TB total right now). I am currently working on my desktop PC (windows 11, 128 RAM, intel i9-12900K CPU, NVIDIA 3090ti GPU, total storage of 14 TB between drives).\\n\\nI plan to drastically increase the amount of training data I will be working with so I need to expand my storage capability and ensure data security and backup. In the future I may want to add more GPUs to train larger models (an I have no idea how that would work with a NAS vs server).\\n\\nWhat do you think the best way to accomplish this would be? I talked with a colleague who uses a Synology NAS for a very similar purpose so I was considering that (maybe a DS423+ or DS923+).\\n\\nAny help would be appreciated.', 'author_fullname': 't2_c3otxvar', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'NAS or server for storing data for ML models [Discussion]', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_18o0rrw', 'quarantine': False, 'link_flair_text_color': None, 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'c30d800e-6abd-11ea-b9f7-0e9770797535', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1703201925.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I am very much an amateur when in comes to ML.&lt;/p&gt;\\n\\n&lt;p&gt;I am currently working on building CNN based classification models (python/pytorch) using large imaging files (digitized histology slides to be exact). Each file is at least 1 gb and I am training the models on thousands of files (about 4 TB total right now). I am currently working on my desktop PC (windows 11, 128 RAM, intel i9-12900K CPU, NVIDIA 3090ti GPU, total storage of 14 TB between drives).&lt;/p&gt;\\n\\n&lt;p&gt;I plan to drastically increase the amount of training data I will be working with so I need to expand my storage capability and ensure data security and backup. In the future I may want to add more GPUs to train larger models (an I have no idea how that would work with a NAS vs server).&lt;/p&gt;\\n\\n&lt;p&gt;What do you think the best way to accomplish this would be? I talked with a colleague who uses a Synology NAS for a very similar purpose so I was considering that (maybe a DS423+ or DS923+).&lt;/p&gt;\\n\\n&lt;p&gt;Any help would be appreciated.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Researcher', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': None, 'id': '18o0rrw', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'V-------', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/MachineLearning/comments/18o0rrw/nas_or_server_for_storing_data_for_ml_models/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/18o0rrw/nas_or_server_for_storing_data_for_ml_models/', 'subreddit_subscribers': 2843382, 'created_utc': 1703201925.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}], 'before': None}}\n",
      "   approved_at_utc        subreddit  \\\n",
      "0             None  MachineLearning   \n",
      "1             None  MachineLearning   \n",
      "2             None  MachineLearning   \n",
      "3             None  MachineLearning   \n",
      "4             None  MachineLearning   \n",
      "5             None  MachineLearning   \n",
      "6             None  MachineLearning   \n",
      "7             None  MachineLearning   \n",
      "8             None  MachineLearning   \n",
      "9             None  MachineLearning   \n",
      "10            None  MachineLearning   \n",
      "11            None  MachineLearning   \n",
      "12            None  MachineLearning   \n",
      "13            None  MachineLearning   \n",
      "14            None  MachineLearning   \n",
      "15            None  MachineLearning   \n",
      "16            None  MachineLearning   \n",
      "17            None  MachineLearning   \n",
      "18            None  MachineLearning   \n",
      "19            None  MachineLearning   \n",
      "20            None  MachineLearning   \n",
      "21            None  MachineLearning   \n",
      "22            None  MachineLearning   \n",
      "23            None  MachineLearning   \n",
      "24            None  MachineLearning   \n",
      "25            None  MachineLearning   \n",
      "\n",
      "                                             selftext author_fullname  saved  \\\n",
      "0   Please post your questions here instead of cre...        t2_6l4z3  False   \n",
      "1   [Nero10578/Mistral-7B-Sunda-v1.0 Â· Hugging Fac...       t2_17brr5  False   \n",
      "2   \"The paper demonstrates the effectiveness of D...       t2_10v86q  False   \n",
      "3                                                         t2_l0o7s29o  False   \n",
      "4   Iâ€™ve been training all my models on the univer...     t2_p3uompls  False   \n",
      "5   **Paper**: [https://arxiv.org/abs/2312.06902](...    t2_mveclxvsc  False   \n",
      "6   I am working on a project that will take PDFs ...     t2_2jgmtz96  False   \n",
      "7   I always seem to get conflicting answers on th...     t2_495cn7pm  False   \n",
      "8   **Paper**: [https://openreview.net/forum?id=vK...    t2_mveclxvsc  False   \n",
      "9   When I ask chatGPT for example about giving me...      t2_e0a821g  False   \n",
      "10  After all the hubbub around the MMLU (for exam...        t2_pm8ge  False   \n",
      "11  I'm training a model for captioning that uses ...    t2_mbc0bpphd  False   \n",
      "12  I was given this assignment but however i have...     t2_a4tygchn  False   \n",
      "13   Hi all , I would like to showcase a simple te...     t2_2rbl6du7  False   \n",
      "14  Can we build an interactive voice robot toy us...     t2_uhxfu9gb  False   \n",
      "15  Looking for those who are knowledgeable in wea...    t2_np7kv7lmp  False   \n",
      "16  I have a project which uses factorization mach...     t2_1h42m1ix  False   \n",
      "17   I am currently working on a project which req...     t2_4f0x63kb  False   \n",
      "18  \\n\\nSuppose i want to train a cat only classif...     t2_4c79bqbe  False   \n",
      "19  Hi everyone, I'm fairly new to the world of ML...     t2_8h09u35i  False   \n",
      "20  I am an attorney who, for better or worse, spe...     t2_3udwh53u  False   \n",
      "21  Was curious about this coding question that I ...     t2_d78s0gv9  False   \n",
      "22  I'm a hobbyist ML researcher and finally, afte...        t2_bkfa9  False   \n",
      "23  The decimator is a function that removes point...     t2_tfe7ylgn  False   \n",
      "24  Hi\\n\\nI am planning to use Colab, but coding i...     t2_tdaoi81o  False   \n",
      "25  Disclaimer: I am very much an amateur when in ...     t2_c3otxvar  False   \n",
      "\n",
      "   mod_reason_title  gilded  clicked  \\\n",
      "0              None       0    False   \n",
      "1              None       0    False   \n",
      "2              None       0    False   \n",
      "3              None       0    False   \n",
      "4              None       0    False   \n",
      "5              None       0    False   \n",
      "6              None       0    False   \n",
      "7              None       0    False   \n",
      "8              None       0    False   \n",
      "9              None       0    False   \n",
      "10             None       0    False   \n",
      "11             None       0    False   \n",
      "12             None       0    False   \n",
      "13             None       0    False   \n",
      "14             None       0    False   \n",
      "15             None       0    False   \n",
      "16             None       0    False   \n",
      "17             None       0    False   \n",
      "18             None       0    False   \n",
      "19             None       0    False   \n",
      "20             None       0    False   \n",
      "21             None       0    False   \n",
      "22             None       0    False   \n",
      "23             None       0    False   \n",
      "24             None       0    False   \n",
      "25             None       0    False   \n",
      "\n",
      "                                                title link_flair_richtext  \\\n",
      "0                         [D] Simple Questions Thread                  []   \n",
      "1   [P] I tried to teach Mistral 7B a new language...                  []   \n",
      "2   [News] Apple Researchers Unveil DeepPCR: A Nov...                  []   \n",
      "3                             [D] Alignment Horseshoe                  []   \n",
      "4   [D] What are some cheap and OK devices for tra...                  []   \n",
      "5   [R] Perseus: Removing Energy Bloat from Large ...                  []   \n",
      "6   [P] Training Local LLM to Translate Text into ...                  []   \n",
      "7   [D]When should and shouldnâ€™t you balance an un...                  []   \n",
      "8   [R] Transformer as a hippocampal memory consol...                  []   \n",
      "9   [D] How can LLMs be aware of the characters ex...                  []   \n",
      "10  [D] Deep dive into the MMLU (\"Are you smarter ...                  []   \n",
      "11  [D] Increase in training loss while in the fir...                  []   \n",
      "12  [P] Fine-Tuning and Evaluating a Falcon 7B/LLA...                  []   \n",
      "13                  [P] Stable Diffusion Telegram Bot                  []   \n",
      "14                       [D] interactive voice robot?                  []   \n",
      "15         [R] Methods for Fault Detection of Sensors                  []   \n",
      "16        [D] Using SparkFM for recommendation system                  []   \n",
      "17  Current State of the Art Face Recognition Mode...                  []   \n",
      "18  [D]How to train a binary classifier with infin...                  []   \n",
      "19                            [D] Advice for Learning                  []   \n",
      "20              [P] Where do I begin for timekeeping?                  []   \n",
      "21           Meta AI Residency Interview Question [D]                  []   \n",
      "22  [P] I built an open SotA image tagging model t...                  []   \n",
      "23  [P] the Decimator, or how to plot a lot of points                  []   \n",
      "24     [D] Pycharm with Google drive (for ML project)                  []   \n",
      "25  NAS or server for storing data for ML models [...                  []   \n",
      "\n",
      "    ...   created_utc  num_crossposts  media is_video  post_hint  \\\n",
      "0   ...  1.702829e+09               0   None    False        NaN   \n",
      "1   ...  1.703242e+09               0   None    False       self   \n",
      "2   ...  1.703248e+09               0   None    False        NaN   \n",
      "3   ...  1.703265e+09               0   None    False        NaN   \n",
      "4   ...  1.703230e+09               0   None    False        NaN   \n",
      "5   ...  1.703250e+09               0   None    False        NaN   \n",
      "6   ...  1.703263e+09               0   None    False        NaN   \n",
      "7   ...  1.703244e+09               0   None    False        NaN   \n",
      "8   ...  1.703251e+09               0   None    False       self   \n",
      "9   ...  1.703249e+09               0   None    False        NaN   \n",
      "10  ...  1.703183e+09               0   None    False        NaN   \n",
      "11  ...  1.703247e+09               0   None    False        NaN   \n",
      "12  ...  1.703268e+09               0   None    False        NaN   \n",
      "13  ...  1.703266e+09               0   None    False       self   \n",
      "14  ...  1.703247e+09               0   None    False        NaN   \n",
      "15  ...  1.703260e+09               0   None    False        NaN   \n",
      "16  ...  1.703255e+09               0   None    False       self   \n",
      "17  ...  1.703252e+09               0   None    False        NaN   \n",
      "18  ...  1.703238e+09               0   None    False        NaN   \n",
      "19  ...  1.703245e+09               0   None    False        NaN   \n",
      "20  ...  1.703261e+09               0   None    False        NaN   \n",
      "21  ...  1.703149e+09               0   None    False        NaN   \n",
      "22  ...  1.703122e+09               2   None    False       self   \n",
      "23  ...  1.703174e+09               0   None    False        NaN   \n",
      "24  ...  1.703226e+09               0   None    False        NaN   \n",
      "25  ...  1.703202e+09               0   None    False        NaN   \n",
      "\n",
      "                                              preview  \\\n",
      "0                                                 NaN   \n",
      "1   {'images': [{'source': {'url': 'https://extern...   \n",
      "2                                                 NaN   \n",
      "3                                                 NaN   \n",
      "4                                                 NaN   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8   {'images': [{'source': {'url': 'https://extern...   \n",
      "9                                                 NaN   \n",
      "10                                                NaN   \n",
      "11                                                NaN   \n",
      "12                                                NaN   \n",
      "13  {'images': [{'source': {'url': 'https://extern...   \n",
      "14                                                NaN   \n",
      "15                                                NaN   \n",
      "16  {'images': [{'source': {'url': 'https://extern...   \n",
      "17                                                NaN   \n",
      "18                                                NaN   \n",
      "19                                                NaN   \n",
      "20                                                NaN   \n",
      "21                                                NaN   \n",
      "22  {'images': [{'source': {'url': 'https://extern...   \n",
      "23                                                NaN   \n",
      "24                                                NaN   \n",
      "25                                                NaN   \n",
      "\n",
      "                                crosspost_parent_list  \\\n",
      "0                                                 NaN   \n",
      "1                                                 NaN   \n",
      "2                                                 NaN   \n",
      "3   [{'approved_at_utc': None, 'subreddit': 'ChatG...   \n",
      "4                                                 NaN   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8                                                 NaN   \n",
      "9                                                 NaN   \n",
      "10                                                NaN   \n",
      "11                                                NaN   \n",
      "12                                                NaN   \n",
      "13                                                NaN   \n",
      "14                                                NaN   \n",
      "15                                                NaN   \n",
      "16                                                NaN   \n",
      "17                                                NaN   \n",
      "18                                                NaN   \n",
      "19                                                NaN   \n",
      "20                                                NaN   \n",
      "21                                                NaN   \n",
      "22                                                NaN   \n",
      "23                                                NaN   \n",
      "24                                                NaN   \n",
      "25                                                NaN   \n",
      "\n",
      "                    url_overridden_by_dest crosspost_parent  \\\n",
      "0                                      NaN              NaN   \n",
      "1                                      NaN              NaN   \n",
      "2                                      NaN              NaN   \n",
      "3   https://www.reddit.com/gallery/18o0590       t3_18o0590   \n",
      "4                                      NaN              NaN   \n",
      "5                                      NaN              NaN   \n",
      "6                                      NaN              NaN   \n",
      "7                                      NaN              NaN   \n",
      "8                                      NaN              NaN   \n",
      "9                                      NaN              NaN   \n",
      "10                                     NaN              NaN   \n",
      "11                                     NaN              NaN   \n",
      "12                                     NaN              NaN   \n",
      "13                                     NaN              NaN   \n",
      "14                                     NaN              NaN   \n",
      "15                                     NaN              NaN   \n",
      "16                                     NaN              NaN   \n",
      "17                                     NaN              NaN   \n",
      "18                                     NaN              NaN   \n",
      "19                                     NaN              NaN   \n",
      "20                                     NaN              NaN   \n",
      "21                                     NaN              NaN   \n",
      "22                                     NaN              NaN   \n",
      "23                                     NaN              NaN   \n",
      "24                                     NaN              NaN   \n",
      "25                                     NaN              NaN   \n",
      "\n",
      "                                       media_metadata  \n",
      "0                                                 NaN  \n",
      "1                                                 NaN  \n",
      "2                                                 NaN  \n",
      "3                                                 NaN  \n",
      "4                                                 NaN  \n",
      "5                                                 NaN  \n",
      "6                                                 NaN  \n",
      "7                                                 NaN  \n",
      "8                                                 NaN  \n",
      "9                                                 NaN  \n",
      "10                                                NaN  \n",
      "11  {'p3nx6xk96u7c1': {'status': 'valid', 'e': 'Im...  \n",
      "12                                                NaN  \n",
      "13                                                NaN  \n",
      "14                                                NaN  \n",
      "15                                                NaN  \n",
      "16                                                NaN  \n",
      "17                                                NaN  \n",
      "18                                                NaN  \n",
      "19                                                NaN  \n",
      "20                                                NaN  \n",
      "21                                                NaN  \n",
      "22                                                NaN  \n",
      "23                                                NaN  \n",
      "24                                                NaN  \n",
      "25                                                NaN  \n",
      "\n",
      "[26 rows x 112 columns]\n",
      "[datetime.datetime(2023, 12, 17, 10, 0, 19), datetime.datetime(2023, 12, 22, 4, 54, 20), datetime.datetime(2023, 12, 22, 6, 19, 33), datetime.datetime(2023, 12, 22, 11, 17, 16), datetime.datetime(2023, 12, 22, 1, 22, 37), datetime.datetime(2023, 12, 22, 6, 59, 44), datetime.datetime(2023, 12, 22, 10, 43, 28), datetime.datetime(2023, 12, 22, 5, 26, 20), datetime.datetime(2023, 12, 22, 7, 11, 22), datetime.datetime(2023, 12, 22, 6, 37, 38), datetime.datetime(2023, 12, 21, 12, 21, 28), datetime.datetime(2023, 12, 22, 6, 9, 54), datetime.datetime(2023, 12, 22, 11, 58, 32), datetime.datetime(2023, 12, 22, 11, 18, 42), datetime.datetime(2023, 12, 22, 6, 14, 8), datetime.datetime(2023, 12, 22, 9, 46, 53), datetime.datetime(2023, 12, 22, 8, 20, 27), datetime.datetime(2023, 12, 22, 7, 26, 11), datetime.datetime(2023, 12, 22, 3, 36, 11), datetime.datetime(2023, 12, 22, 5, 32, 3), datetime.datetime(2023, 12, 22, 10, 3, 2), datetime.datetime(2023, 12, 21, 2, 58, 28), datetime.datetime(2023, 12, 20, 19, 34, 39), datetime.datetime(2023, 12, 21, 9, 54, 54), datetime.datetime(2023, 12, 22, 0, 17, 11), datetime.datetime(2023, 12, 21, 17, 38, 45)]\n",
      "   approved_at_utc        subreddit  \\\n",
      "0             None  MachineLearning   \n",
      "1             None  MachineLearning   \n",
      "2             None  MachineLearning   \n",
      "3             None  MachineLearning   \n",
      "4             None  MachineLearning   \n",
      "5             None  MachineLearning   \n",
      "6             None  MachineLearning   \n",
      "7             None  MachineLearning   \n",
      "8             None  MachineLearning   \n",
      "9             None  MachineLearning   \n",
      "10            None  MachineLearning   \n",
      "11            None  MachineLearning   \n",
      "12            None  MachineLearning   \n",
      "13            None  MachineLearning   \n",
      "14            None  MachineLearning   \n",
      "15            None  MachineLearning   \n",
      "16            None  MachineLearning   \n",
      "17            None  MachineLearning   \n",
      "18            None  MachineLearning   \n",
      "19            None  MachineLearning   \n",
      "20            None  MachineLearning   \n",
      "21            None  MachineLearning   \n",
      "22            None  MachineLearning   \n",
      "23            None  MachineLearning   \n",
      "24            None  MachineLearning   \n",
      "25            None  MachineLearning   \n",
      "\n",
      "                                             selftext author_fullname  saved  \\\n",
      "0   Please post your questions here instead of cre...        t2_6l4z3  False   \n",
      "1   [Nero10578/Mistral-7B-Sunda-v1.0 Â· Hugging Fac...       t2_17brr5  False   \n",
      "2   \"The paper demonstrates the effectiveness of D...       t2_10v86q  False   \n",
      "3                                                         t2_l0o7s29o  False   \n",
      "4   Iâ€™ve been training all my models on the univer...     t2_p3uompls  False   \n",
      "5   **Paper**: [https://arxiv.org/abs/2312.06902](...    t2_mveclxvsc  False   \n",
      "6   I am working on a project that will take PDFs ...     t2_2jgmtz96  False   \n",
      "7   I always seem to get conflicting answers on th...     t2_495cn7pm  False   \n",
      "8   **Paper**: [https://openreview.net/forum?id=vK...    t2_mveclxvsc  False   \n",
      "9   When I ask chatGPT for example about giving me...      t2_e0a821g  False   \n",
      "10  After all the hubbub around the MMLU (for exam...        t2_pm8ge  False   \n",
      "11  I'm training a model for captioning that uses ...    t2_mbc0bpphd  False   \n",
      "12  I was given this assignment but however i have...     t2_a4tygchn  False   \n",
      "13   Hi all , I would like to showcase a simple te...     t2_2rbl6du7  False   \n",
      "14  Can we build an interactive voice robot toy us...     t2_uhxfu9gb  False   \n",
      "15  Looking for those who are knowledgeable in wea...    t2_np7kv7lmp  False   \n",
      "16  I have a project which uses factorization mach...     t2_1h42m1ix  False   \n",
      "17   I am currently working on a project which req...     t2_4f0x63kb  False   \n",
      "18  \\n\\nSuppose i want to train a cat only classif...     t2_4c79bqbe  False   \n",
      "19  Hi everyone, I'm fairly new to the world of ML...     t2_8h09u35i  False   \n",
      "20  I am an attorney who, for better or worse, spe...     t2_3udwh53u  False   \n",
      "21  Was curious about this coding question that I ...     t2_d78s0gv9  False   \n",
      "22  I'm a hobbyist ML researcher and finally, afte...        t2_bkfa9  False   \n",
      "23  The decimator is a function that removes point...     t2_tfe7ylgn  False   \n",
      "24  Hi\\n\\nI am planning to use Colab, but coding i...     t2_tdaoi81o  False   \n",
      "25  Disclaimer: I am very much an amateur when in ...     t2_c3otxvar  False   \n",
      "\n",
      "   mod_reason_title  gilded  clicked  \\\n",
      "0              None       0    False   \n",
      "1              None       0    False   \n",
      "2              None       0    False   \n",
      "3              None       0    False   \n",
      "4              None       0    False   \n",
      "5              None       0    False   \n",
      "6              None       0    False   \n",
      "7              None       0    False   \n",
      "8              None       0    False   \n",
      "9              None       0    False   \n",
      "10             None       0    False   \n",
      "11             None       0    False   \n",
      "12             None       0    False   \n",
      "13             None       0    False   \n",
      "14             None       0    False   \n",
      "15             None       0    False   \n",
      "16             None       0    False   \n",
      "17             None       0    False   \n",
      "18             None       0    False   \n",
      "19             None       0    False   \n",
      "20             None       0    False   \n",
      "21             None       0    False   \n",
      "22             None       0    False   \n",
      "23             None       0    False   \n",
      "24             None       0    False   \n",
      "25             None       0    False   \n",
      "\n",
      "                                                title link_flair_richtext  \\\n",
      "0                         [D] Simple Questions Thread                  []   \n",
      "1   [P] I tried to teach Mistral 7B a new language...                  []   \n",
      "2   [News] Apple Researchers Unveil DeepPCR: A Nov...                  []   \n",
      "3                             [D] Alignment Horseshoe                  []   \n",
      "4   [D] What are some cheap and OK devices for tra...                  []   \n",
      "5   [R] Perseus: Removing Energy Bloat from Large ...                  []   \n",
      "6   [P] Training Local LLM to Translate Text into ...                  []   \n",
      "7   [D]When should and shouldnâ€™t you balance an un...                  []   \n",
      "8   [R] Transformer as a hippocampal memory consol...                  []   \n",
      "9   [D] How can LLMs be aware of the characters ex...                  []   \n",
      "10  [D] Deep dive into the MMLU (\"Are you smarter ...                  []   \n",
      "11  [D] Increase in training loss while in the fir...                  []   \n",
      "12  [P] Fine-Tuning and Evaluating a Falcon 7B/LLA...                  []   \n",
      "13                  [P] Stable Diffusion Telegram Bot                  []   \n",
      "14                       [D] interactive voice robot?                  []   \n",
      "15         [R] Methods for Fault Detection of Sensors                  []   \n",
      "16        [D] Using SparkFM for recommendation system                  []   \n",
      "17  Current State of the Art Face Recognition Mode...                  []   \n",
      "18  [D]How to train a binary classifier with infin...                  []   \n",
      "19                            [D] Advice for Learning                  []   \n",
      "20              [P] Where do I begin for timekeeping?                  []   \n",
      "21           Meta AI Residency Interview Question [D]                  []   \n",
      "22  [P] I built an open SotA image tagging model t...                  []   \n",
      "23  [P] the Decimator, or how to plot a lot of points                  []   \n",
      "24     [D] Pycharm with Google drive (for ML project)                  []   \n",
      "25  NAS or server for storing data for ML models [...                  []   \n",
      "\n",
      "    ... num_crossposts  media  is_video post_hint  \\\n",
      "0   ...              0   None     False       NaN   \n",
      "1   ...              0   None     False      self   \n",
      "2   ...              0   None     False       NaN   \n",
      "3   ...              0   None     False       NaN   \n",
      "4   ...              0   None     False       NaN   \n",
      "5   ...              0   None     False       NaN   \n",
      "6   ...              0   None     False       NaN   \n",
      "7   ...              0   None     False       NaN   \n",
      "8   ...              0   None     False      self   \n",
      "9   ...              0   None     False       NaN   \n",
      "10  ...              0   None     False       NaN   \n",
      "11  ...              0   None     False       NaN   \n",
      "12  ...              0   None     False       NaN   \n",
      "13  ...              0   None     False      self   \n",
      "14  ...              0   None     False       NaN   \n",
      "15  ...              0   None     False       NaN   \n",
      "16  ...              0   None     False      self   \n",
      "17  ...              0   None     False       NaN   \n",
      "18  ...              0   None     False       NaN   \n",
      "19  ...              0   None     False       NaN   \n",
      "20  ...              0   None     False       NaN   \n",
      "21  ...              0   None     False       NaN   \n",
      "22  ...              2   None     False      self   \n",
      "23  ...              0   None     False       NaN   \n",
      "24  ...              0   None     False       NaN   \n",
      "25  ...              0   None     False       NaN   \n",
      "\n",
      "                                              preview  \\\n",
      "0                                                 NaN   \n",
      "1   {'images': [{'source': {'url': 'https://extern...   \n",
      "2                                                 NaN   \n",
      "3                                                 NaN   \n",
      "4                                                 NaN   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8   {'images': [{'source': {'url': 'https://extern...   \n",
      "9                                                 NaN   \n",
      "10                                                NaN   \n",
      "11                                                NaN   \n",
      "12                                                NaN   \n",
      "13  {'images': [{'source': {'url': 'https://extern...   \n",
      "14                                                NaN   \n",
      "15                                                NaN   \n",
      "16  {'images': [{'source': {'url': 'https://extern...   \n",
      "17                                                NaN   \n",
      "18                                                NaN   \n",
      "19                                                NaN   \n",
      "20                                                NaN   \n",
      "21                                                NaN   \n",
      "22  {'images': [{'source': {'url': 'https://extern...   \n",
      "23                                                NaN   \n",
      "24                                                NaN   \n",
      "25                                                NaN   \n",
      "\n",
      "                                crosspost_parent_list  \\\n",
      "0                                                 NaN   \n",
      "1                                                 NaN   \n",
      "2                                                 NaN   \n",
      "3   [{'approved_at_utc': None, 'subreddit': 'ChatG...   \n",
      "4                                                 NaN   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8                                                 NaN   \n",
      "9                                                 NaN   \n",
      "10                                                NaN   \n",
      "11                                                NaN   \n",
      "12                                                NaN   \n",
      "13                                                NaN   \n",
      "14                                                NaN   \n",
      "15                                                NaN   \n",
      "16                                                NaN   \n",
      "17                                                NaN   \n",
      "18                                                NaN   \n",
      "19                                                NaN   \n",
      "20                                                NaN   \n",
      "21                                                NaN   \n",
      "22                                                NaN   \n",
      "23                                                NaN   \n",
      "24                                                NaN   \n",
      "25                                                NaN   \n",
      "\n",
      "                    url_overridden_by_dest  crosspost_parent  \\\n",
      "0                                      NaN               NaN   \n",
      "1                                      NaN               NaN   \n",
      "2                                      NaN               NaN   \n",
      "3   https://www.reddit.com/gallery/18o0590        t3_18o0590   \n",
      "4                                      NaN               NaN   \n",
      "5                                      NaN               NaN   \n",
      "6                                      NaN               NaN   \n",
      "7                                      NaN               NaN   \n",
      "8                                      NaN               NaN   \n",
      "9                                      NaN               NaN   \n",
      "10                                     NaN               NaN   \n",
      "11                                     NaN               NaN   \n",
      "12                                     NaN               NaN   \n",
      "13                                     NaN               NaN   \n",
      "14                                     NaN               NaN   \n",
      "15                                     NaN               NaN   \n",
      "16                                     NaN               NaN   \n",
      "17                                     NaN               NaN   \n",
      "18                                     NaN               NaN   \n",
      "19                                     NaN               NaN   \n",
      "20                                     NaN               NaN   \n",
      "21                                     NaN               NaN   \n",
      "22                                     NaN               NaN   \n",
      "23                                     NaN               NaN   \n",
      "24                                     NaN               NaN   \n",
      "25                                     NaN               NaN   \n",
      "\n",
      "                                       media_metadata   formatted_created  \n",
      "0                                                 NaN 2023-12-17 10:00:19  \n",
      "1                                                 NaN 2023-12-22 04:54:20  \n",
      "2                                                 NaN 2023-12-22 06:19:33  \n",
      "3                                                 NaN 2023-12-22 11:17:16  \n",
      "4                                                 NaN 2023-12-22 01:22:37  \n",
      "5                                                 NaN 2023-12-22 06:59:44  \n",
      "6                                                 NaN 2023-12-22 10:43:28  \n",
      "7                                                 NaN 2023-12-22 05:26:20  \n",
      "8                                                 NaN 2023-12-22 07:11:22  \n",
      "9                                                 NaN 2023-12-22 06:37:38  \n",
      "10                                                NaN 2023-12-21 12:21:28  \n",
      "11  {'p3nx6xk96u7c1': {'status': 'valid', 'e': 'Im... 2023-12-22 06:09:54  \n",
      "12                                                NaN 2023-12-22 11:58:32  \n",
      "13                                                NaN 2023-12-22 11:18:42  \n",
      "14                                                NaN 2023-12-22 06:14:08  \n",
      "15                                                NaN 2023-12-22 09:46:53  \n",
      "16                                                NaN 2023-12-22 08:20:27  \n",
      "17                                                NaN 2023-12-22 07:26:11  \n",
      "18                                                NaN 2023-12-22 03:36:11  \n",
      "19                                                NaN 2023-12-22 05:32:03  \n",
      "20                                                NaN 2023-12-22 10:03:02  \n",
      "21                                                NaN 2023-12-21 02:58:28  \n",
      "22                                                NaN 2023-12-20 19:34:39  \n",
      "23                                                NaN 2023-12-21 09:54:54  \n",
      "24                                                NaN 2023-12-22 00:17:11  \n",
      "25                                                NaN 2023-12-21 17:38:45  \n",
      "\n",
      "[26 rows x 113 columns]\n"
     ]
    }
   ],
   "source": [
    "# ETL - Extract, Transform, Load\n",
    "# Get data from multiple sources (Databases, files, API, etc.)\n",
    "# Transform the data to be useable - (weather e.x.: T for rain - (trace), near nothing).\n",
    "# Load data - store / save our data, often to a database or a file.\n",
    "\n",
    "import pandas as pd\n",
    "from requests import get, HTTPError, ConnectionError\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "reddit_endpoint = 'https://www.reddit.com/r/machinelearning/.json'\n",
    "json_data = None\n",
    "try:\n",
    "    req = get(reddit_endpoint, headers = {\"User-agent\": \"max-etl-pipeline\"})\n",
    "    json_data = req.json()\n",
    "except (HTTPError, ConnectionError) as err:\n",
    "    print(err)\n",
    "\n",
    "print(json_data)\n",
    "with open(\"data.json\", \"w\") as outfile:\n",
    "    json.dump(json_data,outfile,indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "children = json_data.get(\"data\", {}).get(\"children\", [])\n",
    "data = [child.get('data', {}) for child in children]\n",
    "df= pd.DataFrame(data)\n",
    "print(df)\n",
    "titles = df[\"title\"].tolist()\n",
    "num_comments = df[\"num_comments\"].tolist()\n",
    "created = df[\"created\"].tolist()\n",
    "formatted_dates = [datetime.datetime.fromtimestamp(timestamp) for timestamp in created]\n",
    "df[\"formatted_created\"] = formatted_dates\n",
    "print(formatted_dates)\n",
    "upvote_ratio = df[\"upvote_ratio\"].tolist()\n",
    "ups = df[\"ups\"].tolist()\n",
    "downs = df[\"downs\"].tolist()\n",
    "\n",
    "print(df)\n",
    "selected_columns = [\"title\", \"num_comments\", \"formatted_created\", \"upvote_ratio\", \"ups\", \"downs\"]\n",
    "selected_df = df[selected_columns]\n",
    "\n",
    "\n",
    "selected_df.to_csv(\"reddit_data.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Your company wants you to get relevant data from that endpoint - ensure that data is easily useable.\n",
    "# Make a CSV file with all relevant data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
